
## If you want to force CPU use instead of GPU
#import os
#os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"   # see issue #152
#os.environ["CUDA_VISIBLE_DEVICES"] = ""

import numpy as np
import pandas as pd
import re
from bs4 import BeautifulSoup
import random

## Setting random states
np.random.seed(123)
random.seed(123)

## Configuration for GPU limits:
from keras import backend as K
if 'tensorflow' == K.backend():
    import tensorflow as tf
    print(tf.__version__)
    print(K.tensorflow_backend._get_available_gpus())
else:
    import theano as tf
    print(tf.__version__)
    print(K.tensorflow_backend._get_available_gpus())

## Setting random states
tf.set_random_seed(123)
from keras.backend.tensorflow_backend import set_session
config = tf.ConfigProto()
print("*** Before setting allow_growth:")
print(config.gpu_options.per_process_gpu_memory_fraction)
config.gpu_options.per_process_gpu_memory_fraction = 0.8
config.gpu_options.allow_growth = True
config.gpu_options.visible_device_list = "0"
print("*** After setting allow_growth:")
print(config.gpu_options.per_process_gpu_memory_fraction)
#session = tf.Session(config=config)
set_session(tf.Session(config=config))
## Check if GPU is being used:
from tensorflow.python.client import device_lib
print("*** Listing devices:")
print(device_lib.list_local_devices())


from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical as to_cat
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional
from keras.models import Model
from keras.engine.topology import Layer, InputSpec
from keras import initializers, regularizers
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.utils import shuffle
import pickle
from sklearn import metrics



# Force TensorFlow to use single thread.
# Multiple threads are a potential source of
# non-reproducible results.
# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res
# session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)



EMBEDDING_DIM = 300
GLOVEFILE = "../pretrained/Gloved-GoogleNews-vectors-negative300.txt"#../pretrained/glove.6B.100d.txt"): ## "../pretrained/Gloved-GoogleNews-vectors-negative300.txt"):
MAX_SEQUENCE_LENGTH = 1000
MAX_NB_WORDS = 20000


CLASSES = 5
EPOCS = 20
BATCHSIZE = 64
USEKERAS = True
LOAD_DATA_FROM_DISK = True
RUNS = 1




def prepare_cnn_model_1(word_index, embedding_matrix):
   embedding_layer = Embedding(len(word_index) + 1,
                               EMBEDDING_DIM,
                               weights=[embedding_matrix],
                               input_length=MAX_SEQUENCE_LENGTH,
                               trainable=True)
   sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
   embedded_sequences = embedding_layer(sequence_input)
   l_cov1 = Conv1D(64, 3, activation='relu')(embedded_sequences)
   l_pool1 = MaxPooling1D()(l_cov1)
   #l_dropout1 = Dropout(0.5)(l_pool1)
   #l_cov2 = Conv1D(64, 3, activation='relu')(l_pool1)
   #l_pool2 = MaxPooling1D()(l_cov2)
   l_flat = Flatten()(l_pool1)
   l_dense = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.1))(l_flat)
   l_dropout2 = Dropout(0.5)(l_dense)
   preds = Dense(CLASSES, activation='softmax')(l_dropout2)
   model = Model(sequence_input, preds)
   model.compile(loss='categorical_crossentropy',
                 optimizer='rmsprop',
                 metrics=['acc'])
   return model


def prepare_cnn_model_2(word_index, embedding_matrix):
    embedding_layer = Embedding(len(word_index) + 1,
                                EMBEDDING_DIM,
                                weights=[embedding_matrix],
                                input_length=MAX_SEQUENCE_LENGTH,
                                trainable=True)
    convs = []
    filter_sizes = [2, 3, 5]
    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
    embedded_sequences = embedding_layer(sequence_input)
    for fsz in filter_sizes:
        l_conv = Conv1D(nb_filter=128, filter_length=fsz, activation='relu')(embedded_sequences)
        l_pool = MaxPooling1D(5)(l_conv)
        convs.append(l_pool)
    l_merge = Merge(mode='concat', concat_axis=1)(convs)
    l_cov1 = Conv1D(128, 5, activation='relu')(l_merge)
    l_pool1 = MaxPooling1D(5)(l_cov1)
    l_dropout1 = Dropout(0.5)(l_pool1)
    l_cov2 = Conv1D(128, 5, activation='relu')(l_dropout1)
    l_pool2 = MaxPooling1D(30)(l_cov2)
    l_flat = Flatten()(l_pool2)
    l_dense = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.2))(l_flat)
    l_dropout2 = Dropout(0.5)(l_dense)
    preds = Dense(CLASSES, activation='softmax')(l_dropout2)
    model = Model(sequence_input, preds)
    model.compile(loss='categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['acc'])
    return model





def clean_str(string):
    """
    Tokenization/string cleaning for dataset
    Every dataset is lower cased except
    """
    string = re.sub(r"\\", "", string.decode("utf-8"))
    string = re.sub(r"\'", "", string.decode("utf-8"))
    string = re.sub(r"\"", "", string.decode("utf-8"))
    string = ''.join(e for e in string if (e.isspace() or e.isalnum()))
    return string.strip().lower()

def load_data_imdb():
    print("Loading data...")
    data_train = pd.read_csv('../data/imdbReviews/labeledTrainData.tsv', sep='\t')
    print(data_train.shape)
    texts = []
    labels = []
    for idx in range(data_train.review.shape[0]):
        text = BeautifulSoup(data_train.review[idx])
        texts.append(clean_str(text.get_text().encode('ascii', 'ignore')))
        labels.append(data_train.sentiment[idx])
    labels = to_cat(np.asarray(labels))

    return texts, labels

def load_data_liar(file_name):
    print("Loading data...")
    data_train = pd.read_table(file_name, sep='\t', header=None, names=["id", "label","data"], usecols=[0,1,2])
    print(data_train.shape)
    texts = []
    labels = []
    for idx in range(data_train.data.shape[0]):
        text = BeautifulSoup(data_train.data[idx])
        texts.append(clean_str(text.get_text().encode('ascii', 'ignore')))
        labels.append(data_train.label[idx])
    transdict = {
        'true': 0,
        'mostly-true': 0,
        'half-true': 0,
        'barely-true': 1,
        'false': 1,
        'pants-fire': 1
    }
    labels = [transdict[i] for i in labels]
    labels = to_cat(np.asarray(labels))
    print(texts[0:6])
    print(labels[0:6])
    return texts, labels



def load_data_combined(file_name = "../data/buzzfeed-debunk-combined/all-v02.txt"):
    print("Loading data...")
    data_train = pd.read_table(file_name, sep='\t', header= None, names=["id",	"url",	"label", "data", "domain", "source"],usecols=[2,3])
    print(data_train.shape)
    print(data_train.label[0:10])
    print(data_train.label.unique())
    #print(data_train[data_train["label"].isnull()])
    texts = []
    labels = []
    for idx in range(data_train.data.shape[0]):
        text = BeautifulSoup(data_train.data[idx])
        texts.append(clean_str(text.get_text().encode('ascii', 'ignore')))
        labels.append(data_train.label[idx])
    if( CLASSES == 2):
        transdict = {
            'ftrue': 0,
            'mtrue': 0,
            'mfalse': 1,
            'ffalse': 1,

            'mixture': 4,
            'pantsfire': 5,
            'nofact': 6
        }
    else:
        transdict = {
            'ftrue': 0,
            'mtrue': 1,
            'mixture': 2 ,
            'mfalse':3,
            'ffalse': 4,

            'pantsfire': 5,
            'nofact': 6
        }
    labels = [transdict[i] for i in labels]
    #labels = to_cat(np.asarray(labels))
    print(labels[0:6])
    return texts, labels

def load_data_rashkin(file_name = "../data/rashkin/train.txt"):
    print("Loading data...")
    data_train = pd.read_table(file_name, sep='\t',  header= None, names=["label", "data"], usecols=[0,1], dtype = {"label": np.str, "data": np.str})
    print(data_train.shape)
    print(data_train[0:6])
    texts = []
    labels = []
    #for i in range(data_train.data.shape[0]):
    #   print(i, type(data_train.data[i]))
    for idx in range(data_train.data.shape[0]):
        text = BeautifulSoup(data_train.data[idx])
        texts.append(clean_str(text.get_text().encode('ascii', 'ignore')))
        labels.append(str(data_train.label[idx]))
    transdict = {
        '1': 2, #Satire
        '2': 3, #Hoax
        '3': 1, #Propaganda
        '4': 0  #Truested
    }
    labels = [transdict[i] for i in labels]
    labels = to_cat(np.asarray(labels))
    print(texts[0:6])
    print(labels[0:6])
    return texts, labels



def load_data_buzzfeed(file_name = "../data/buzzfeed-facebook/bf_fb.txt"):
    print("Loading data...")
    data_train = pd.read_table(file_name, sep='\t', header= None, names=["ID",	"URL",	"label",	"data",	"error"], usecols=[2,3])
    print(data_train.shape)
    texts = []
    labels = []
    for idx in range(data_train.data.shape[0]):
        text = BeautifulSoup(data_train.data[idx])
        texts.append(clean_str(text.get_text().encode('ascii', 'ignore')))
        labels.append(data_train.label[idx])
    transdict = {
        'no factual content': 0,
        'mostly true': 1,
        'mixture of true and false': 2,
        'mostly false': 3
    }
    labels = [transdict[i] for i in labels]
    labels = to_cat(np.asarray(labels))
    print(texts[0:6])
    print(labels[0:6])
    return texts, labels



def balance_data(texts, labels, sample_size, discard_labels = [] ):
    ## sample size is the number of items we want to have from EACH class
    unique, counts = np.unique(labels, return_counts=True)
    print(np.asarray((unique, counts)).T)
    all_index = np.empty([0], dtype=int)
    for l, f in zip(unique, counts):
        if (l in discard_labels ):
            print ("Discarding items for label " + str( l))
            continue
        l_index = (np.where( labels ==  l )[0]).tolist()  ## index of input data with current label
        if( sample_size - f > 0 ):
            #print "Upsampling ", sample_size - f, " items for class ", l
            x = np.random.choice(f, sample_size - f).tolist()
            l_index = np.append(np.asarray(l_index) , np.asarray(l_index)[x])
        else:
            #print "Downsampling ", sample_size , " items for class ", l
            l_index = random.sample(l_index, sample_size)
        all_index = np.append(all_index, l_index)
    bal_labels = np.asarray(labels)[all_index.tolist()]
    bal_texts = np.asarray(texts)[all_index.tolist()]
    remaining = [i for i in range(0, np.sum(counts)) if i not in all_index.tolist()]
    rem_texts = np.asarray(texts)[remaining]
    rem_labels = np.asarray(labels)[remaining]
    print ("Final size of dataset:")
    unique, counts = np.unique(bal_labels, return_counts=True)
    print (np.asarray((unique, counts)).T)
    print ("Final size of remaining dataset:")
    unique, counts = np.unique(rem_labels, return_counts=True)
    print (np.asarray((unique, counts)).T)
    return bal_texts, bal_labels, rem_texts, rem_labels





def sequence_processing(texts):
    """
    word indexing and padding of the sequences
    """

    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
    tokenizer.fit_on_texts(texts)
    sequences = tokenizer.texts_to_sequences(texts)
    word_index = tokenizer.word_index
    print('Found %s unique tokens.' % len(word_index))
    texts = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
    return texts, word_index


def load_embeddings( word_index , embedding_file = GLOVEFILE):
   print("Loading embeddings...")
   embeddings_index = {}
   f = open(embedding_file)
   for line in f:
       values = line.split()
       word = values[0]
       coefs = np.asarray(values[1:], dtype='float32')
       embeddings_index[word] = coefs
   f.close()
   print('Total %s word vectors in file.' % len(embeddings_index))

   embedding_matrix = np.random.random((len(word_index ) + 1, EMBEDDING_DIM))
   for word, i in word_iecols=[0,1], dtype = {"label": np.str, "data": np.str})
    print(data_train.shape)
    print(data_train[0:6])
    texts = []
    labels = []
    #for i in range(data_train.data.shape[0]):
    #   print(i, type(data_train.data[i]))
    for idx in range(data_train.data.shape[0]):
        text = BeautifulSoup(data_train.data[idx])
        texts.append(clean_str(text.get_text().encode('ascii', 'ignore')))
        labels.append(str(data_train.label[idx]))
    transdict = {
        '1': 2, #Satire
        '2': 3, #Hoax
        '3': 1, #Propaganda
        '4': 0  #Truested
    }
    labels = [transdict[i] for i in labels]
    labels = to_cat(np.asarray(labels))
    print(texts[0:6])
    print(labels[0:6])
    return texts, labels



def load_data_buzzfeed(file_name = "../data/buzzfeed-facebook/bf_fb.txt"):
    print("Loading data...")
    data_train = pd.read_table(file_name, sep='\t', header= None, names=["ID",	"URL",	"label",	"data",	"error"], usecols=[2,3])
    print(data_train.shape)
    texts = []
    labels = []
    for idx in range(data_train.data.shape[0]):
        text = BeautifulSoup(data_train.data[idx])
        texts.append(clean_str(text.get_text().encode('ascii', 'ignore')))
        labels.append(data_train.label[idx])
    transdict = {
        'no factual content': 0,
        'mostly true': 1,
        'mixture of true and false': 2,
        'mostly false': 3
    }
    labels = [transdict[i] for i in labels]
    labels = to_cat(np.asarray(labels))
    print(texts[0:6])
    print(labels[0:6])
    return texts, labels



def balance_data(texts, labels, sample_size, discard_labels = [] ):
    ## sample size is the number of items we want to have from EACH class
    unique, counts = np.unique(labels, return_counts=True)
    print(np.asarray((unique, counts)).T)
    all_index = np.empty([0], dtype=int)
    for l, f in zip(unique, counts):
        if (l in discard_labels ):
            print ("Discarding items for label " + str( l))
            continue
        l_index = (np.where( labels ==  l )[0]).tolist()  ## index of input data with current label
        if( sample_size - f > 0 ):
            #print "Upsampling ", sample_size - f, " items for class ", l
            x = np.random.choice(f, sample_size - f).tolist()
            l_index = np.append(np.asarray(l_index) , np.asarray(l_index)[x])
        else:
            #print "Downsampling ", sample_size , " items for class ", l
            l_index = random.sample(l_index, sample_size)
        all_index = np.append(all_index, l_index)
    bal_labels = np.asarray(labels)[all_index.tolist()]
    bal_texts = np.asarray(texts)[all_index.tolist()]
    remaining = [i for i in range(0, np.sum(counts)) if i not in all_index.tolist()]
    rem_texts = np.asarray(texts)[remaining]
    rem_labels = np.asarray(labels)[remaining]
    print ("Final size of dataset:")
    unique, counts = np.unique(bal_labels, return_counts=True)
    print (np.asarray((unique, counts)).T)
    print ("Final size of remaining dataset:")
    unique, counts = np.unique(rem_labels, return_counts=True)
    print (np.asarray((unique, counts)).T)
    return bal_texts, bal_labels, rem_texts, rem_labels





def sequence_processing(texts):
    """
    word indexing and padding of the sequences
    """

    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
    tokenizer.fit_on_texts(texts)
    sequences = tokenizer.texts_to_sequences(texts)
    word_index = tokenizer.word_index
    print('Found %s unique tokens.' % len(word_index))
    texts = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
    return texts, word_index


def load_embeddings( word_index , embedding_file = GLOVEFILE):
   print("Loading embeddings...")
   embeddings_index = {}
   f = open(embedding_file)
   for line in f:
       values = line.split()
       word = values[0]
       coefs = np.asarray(values[1:], dtype='float32')
       embeddings_index[word] = coefs
   f.close()
   print('Total %s word vectors in file.' % len(embeddings_index))

   embedding_matrix = np.random.random((len(word_index ) + 1, EMBEDDING_DIM))
   for word, i in word_i