

## Used ideas from https://www.kaggle.com/metadist/work-like-a-pro-with-pipelines-and-feature-unions
## Used ideas from https://www.kaggle.com/edolatabadi/feature-union-with-grid-search
## Used ideas from https://github.com/scikit-learn/scikit-learn/issues/6122 feature selection output



from __future__ import print_function

import numpy as np

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.datasets import fetch_20newsgroups
from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer
from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from collections import Counter
from sklearn.model_selection import cross_validate

from textutils import DataLoading
import os
import numpy as np
from nltk.corpus import stopwords
import nltk
import pandas as pd
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import GridSearchCV
import string
import textstat
import textblob

#################

print("Preparing lexicons & lwicDic")
lexicon_directory = "../data/bias_related_lexicons"
lexicons = []
lexiconNames = []
# print("LexiconFeatures() init: loading lexicons")
for filename in os.listdir(lexicon_directory):
    if filename.endswith(".txt"):
        file = os.path.join(lexicon_directory, filename)
        words = open(file, encoding = "ISO-8859-1").read()
        lexicon = {k: 0 for k in nltk.word_tokenize(words)}
        print("Number of terms in the lexicon " + filename + " : " + str(len(lexicon)))
        lexicons.append(lexicon)
        lexiconNames.append(filename)
        continue
    else:
        continue

liwcFile = "../data/lwic/vocabliwc_cats.csv"
cols = list(pd.read_csv(liwcFile, nrows=1))
df = pd.read_csv(liwcFile, index_col="Source (A)",
                 usecols=[i for i in cols if i not in ['WC', 'Analytic', 'Clout', 'Authentic', 'Tone', 'WPS']])
df = df.T
keys = df.index
df = df.reset_index().drop('index', axis='columns')
cols = df.columns
values = df.apply(lambda x: x > 0).apply(lambda x: list(cols[x.values]), axis=1)
liwcDic = dict(zip(keys, values))

#####################
class PosTagFeatures(BaseEstimator, TransformerMixin):
    pos_family = {}

    def __init__(self):
        print("Inside the init function of PosTagFeatures()")

    # fit() doesn't do anything, this is a transformer class
    def fit(self, texts, y=None):
        return self

    # all the work is done here
    def transform(self, texts):
        allTags = ['NOUN', 'PRON', 'ADJ', 'ADV', 'VERB', 'ADP', 'NUM', 'PRT', 'DET', 'X', 'CONJ', '.']
        tokenizer = lambda x: x.split()
        features = [dict(Counter(allTags + [tag for word, tag in nltk.pos_tag(tokenizer(text), tagset='universal')]))
                    for text in texts]
        # normalize by the number of all tags (words in the text + 12 smoothing factor)
        features = [{key: val / (len(text.split()) + 12) for key, val in d.items()} for text, d in zip(texts, features)]
        features = np.array(features)
        return features


class SurfaceFeatures(BaseEstimator, TransformerMixin):
    """Extract features from each document for DictVectorizer"""
    XXX = None

    def __init__(self):
        self.XXX = "Inside the init function of SurfaceFeatures()"
        print(self.XXX)

    def fit(self, x, y=None):
        return self

    def transform(self, posts):
        # posts[['feature1','feature2',...,'feature100']].to_dict('records')[0].to_dict('records')
        features = [{'num_char': len(text),
                     'num_sentence': text.count('.'),
                     'num_punc/num_char': len("".join(_ for _ in text if _ in string.punctuation)) / (len(text) + 1),
                     'num_upper/num_char': len([wrd for wrd in text.split() if wrd.isupper()]) / (len(text) + 1),
                     'num_word/num_sentence': len(text.split()) / (text.count('.') + 1)
                     }
                    for text in posts]
        return features


class LiwcFeatures(BaseEstimator, TransformerMixin):
    liwcDic = {}  # = map of text to dataframe row (this dataframe should be read from the file including liwc features)

    def __init__(self):
        print("Inside the init function of LiwcFeatures()")
        self.liwcDic = liwcDic

    def fit(self, x, y=None):
        return self

    def transform(self, texts):
        textvecs = []
        for text in texts:
            # print "*** Current text:\n" + text  + "\n***"
            tokens = nltk.word_tokenize(text)
            textvec = {}
            for category in self.liwcDic.keys():
                lexicon_words = [i for i in tokens if i in self.liwcDic[category]]
                count = len(lexicon_words)
                count = count * 1.0 / len(tokens)  # Continous treatment
                # count = 1 if (count > 0) else 0     #Binary treatment
                textvec[category] = count
            textvecs.append(textvec)
        textvecs = np.array(textvecs)
        return textvecs


class LexiconFeatures(BaseEstimator, TransformerMixin):
    lexicons = None
    lexiconNames = None

    def __init__(self):
        print("Inside the init function of LexiconFeatures()")
        self.lexicons = lexicons
        self.lexiconNames = lexiconNames

    def fit(self, x, y=None):
        return self

    def transform(self, texts):
        stoplist = stopwords.words('english')
        textvecs = []
        for text in texts:
            tokens = nltk.word_tokenize(text)
            textvec = {}
            for lexicon, lexiconName in zip(self.lexicons, self.lexiconNames):
                lexicon_words = [i for i in tokens if i in lexicon]
                count = len(lexicon_words)
                count = count * 1.0 / len(tokens)  # Continous treatment
                # count = 1 if (count > 0) else 0     #Binary treatment
                textvec[lexiconName] = count
            textvecs.append(textvec)
        textvecs = np.array(textvecs)
        return textvecs


class ReadabilityFeatures(BaseEstimator, TransformerMixin):
    def __init__(self):
        print("Inside the init function of readabilityFeatures()")

    def fit(self, x, y=None):
        return self

    def transform(self, posts):
        # posts[['feature1','feature2',...,'feature100']].to_dict('records')[0].to_dict('records')
        features = [{'flesch_reading_ease': textstat.flesch_reading_ease(text),
                     'smog_index': textstat.smog_index(text),
                     'flesch_kincaid_grade': textstat.flesch_kincaid_grade(text),
                     'coleman_liau_index': textstat.coleman_liau_index(text),
                     'automated_readability_index': textstat.automated_readability_index(text),
                     'dale_chall_readability_score': textstat.dale_chall_readability_score(text),
                     'linsear_write_formula': textstat.linsear_write_formula(text),
                     'gunning_fog': textstat.gunning_fog(text)
                     # 'text_standard': textstat.text_standard(text)
                     }
                    for text in posts]
        return features


class SubjectBodyExtractor(BaseEstimator, TransformerMixin):
    def fit(self, x, y=None):
        return self

    def transform(self, posts):
        # construct object dtype array with two columns
        # first column = 'subject' and second column = 'body'
        features = np.empty(shape=(len(posts), 2), dtype=object)
        for i, text in enumerate(posts):
            features[i, 0] = text[0:100]
            features[i, 1] = text
        return features



#################################

LOAD_DATA_FROM_DISK = True
CLASSES = 2

if LOAD_DATA_FROM_DISK:
    texts_train = np.load("../dump/trainRaw")
    texts_valid = np.load("../dump/validRaw")
    texts_test = np.load("../dump/testRaw")
    labels_train = np.load("../dump/trainlRaw")
    labels_valid = np.load("../dump/validlRaw")
    labels_test = np.load("../dump/testlRaw")
    print("Data loaded from disk!")

else:
    # Data sources used for training:
    texts_snopes, labels_snopes = DataLoading.load_data_snopes \
        ("../data/snopes/snopes_leftover_v02_right_forclassificationtrain.csv", CLASSES  )  # load_data_liar("../data/liar_dataset/train.tsv")#load_data_rashkin("../data/r$
    texts_buzzfeed, labels_buzzfeed = DataLoading.load_data_buzzfeed("../data/buzzfeed-facebook/buzzfeed-v02-originalLabels.txt",
                                                                                 CLASSES)
    texts_emergent, labels_emergent = DataLoading.load_data_emergent(
        "../data/emergent/url-versions-2015-06-14.csv", CLASSES)
    len(texts_snopes)
    len(texts_buzzfeed)
    len(texts_emergent)

    texts_snopes, labels_snopes, texts, labels = DataLoading.balance_data(texts_snopes, labels_snopes, 259, [2, 3, 4, 5])
    texts_buzzfeed, labels_buzzfeed, texts, labels = DataLoading.balance_data(texts_buzzfeed, labels_buzzfeed, 64, [2, 3, 4, 5])
    texts_emergent, labels_emergent, texts, labels = DataLoading.balance_data(texts_emergent,labels_emergent, 0, [2, 3, 4, 5])

    texts_all = pd.concat(
        [pd.Series(texts_snopes), pd.Series(texts_buzzfeed), pd.Series(texts_emergent)])
    labels_all = pd.concat(
        [pd.Series(labels_snopes), pd.Series(labels_buzzfeed), pd.Series(labels_emergent)])


    texts_train, labels_train, texts, labels = DataLoading.balance_data(texts_all, labels_all, 323,
                                                                        [2, 3, 4, 5])
    texts_valid, labels_valid, texts, labels = DataLoading.balance_data(texts, labels, 0  , [2, 3, 4, 5])
    texts_test, labels_test, texts, labels = DataLoading.balance_data(texts, labels, 0, [2, 3, 4, 5])
    texts_train.dump("../dump/trainRaw")
    texts_valid.dump("../dump/validRaw")
    texts_test.dump("../dump/testRaw")
    labels_train.dump("../dump/trainlRaw")
    labels_valid.dump("../dump/validlRaw")
    labels_test.dump("../dump/testlRaw")
    print("Data dumped to disk!")

print("Size of train, validataion and test sets: " + str(len(labels_train)) + " , " + str(
    len(labels_valid)) + " , " + str(len(labels_test)))
texts_train_valid = pd.concat(
    [pd.Series(texts_train), pd.Series(texts_valid)])
labels_train_valid = pd.concat(
    [pd.Series(labels_train), pd.Series(labels_valid)])
print(texts_train_valid[0:3][0:10])
print(labels_train_valid[0:3])



# Train & Test Loop

tws = [ {'body_bow': 0.2,
            'surface_features': 0.0,
            'lexicon_features': 0.2,
            'liwc_features' :0.2,
            'pos_features':0.0,
            'readability_features': 0.0},
        {'body_bow': 0.2,
            'surface_features': 0.2,
            'lexicon_features': 0.2,
            'liwc_features' :0.2,
            'pos_features':0.2,
            'readability_features': 0.2},
        {'body_bow': 0.2,
         'surface_features': 0.0,
         'lexicon_features': 0.0,
         'liwc_features': 0.0,
         'pos_features': 0.0,
         'readability_features': 0.0},
        {'body_bow': 0.0,
            'surface_features': 0.2,
            'lexicon_features': 0.0,
            'liwc_features' :0.0,
            'pos_features':0.0,
            'readability_features': 0.0},
        {'body_bow': 0.0,
            'surface_features': 0.0,
            'lexicon_features': 0.2,
            'liwc_features' :0.0,
            'pos_features':0.0,
            'readability_features': 0.0},
        {'body_bow': 0.0,
            'surface_features': 0.0,
            'lexicon_features': 0.0,
            'liwc_features' :0.2,
            'pos_features':0.0,
            'readability_features': 0.0},
        {'body_bow': 0.0,
            'surface_features': 0.0,
            'lexicon_features': 0.0,
            'liwc_features' :0.0,
            'pos_features':0.2,
            'readability_features': 0.0},
        {'body_bow': 0.0,
            'surface_features': 0.0,
            'lexicon_features': 0.0,
            'liwc_features' :0.0,
            'pos_features':0.0,
            'readability_features': 0.2}
]

texts_snopesChecked, labels_snopesChecked = DataLoading.load_data_snopes("../data/snopes/snopes_checked_v02_right_forclassificationtest.csv",CLASSES)
texts_buzzfeedTop, labels_buzzfeedTop = DataLoading.load_data_buzzfeedtop()
texts_perez, labels_perez = DataLoading.load_data_perez("../data/perez/celeb.csv")

for tw in tws:
    print("\n*** Feature weigths *** \n", tw)
    pipeline = Pipeline([
        # Extract the subject & body
        ('subjectbody', SubjectBodyExtractor()),

        # Use ColumnTransformer to combine the features from subject and body
        ('union', ColumnTransformer(
            [
                ('body_bow', TfidfVectorizer(sublinear_tf=True, max_df=0.5, min_df=5, ngram_range=(1 ,2),
                                             stop_words='english'), 1),
                ('pos_features', Pipeline([
                    ('stats', PosTagFeatures()),  # returns a list of dicts
                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix
                ]), 1),
                ('surface_features', Pipeline([
                    ('stats', SurfaceFeatures()),  # returns a list of dicts
                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix
                ]), 1),
                ('lexicon_features', Pipeline([
                    ('stats', LexiconFeatures()),  # returns a list of dicts
                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix
                ]), 1),
                ('liwc_features', Pipeline([
                    ('stats', LiwcFeatures()),  # returns a list of dicts
                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix
                    ]), 1),
                ('readability_features', Pipeline([
                    ('stats', ReadabilityFeatures()),  # returns a list of dicts
                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix
                    ]), 1)
            ],
            transformer_weights=tw
        )),
        ('svc', LinearSVC(penalty= "l2", dual=False, tol=1e-3)),
    ])


    print("Fitting the model...")

    cross_val = cross_validate(pipeline, texts_train_valid, labels_train_valid, return_train_score=True, cv=3, scoring = 'f1_micro')
    print("********************\n\n")
    print(cross_val, "\n Mean train score: ", np.mean(cross_val['train_score']), "\n Mean test score: ", np.mean(cross_val['test_score']))
    print("********************\n\n")
    grid_search = pipeline.fit(texts_train_valid, labels_train_valid)
    print("Results on training data:")
    y = grid_search.predict(texts_train_valid)
    print(classification_report(labels_train_valid, y))

    print("Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):")
    texts_test, labels_test, texts, labels = DataLoading.balance_data(texts_snopesChecked, labels_snopesChecked ,sample_size=None , discard_labels=[2,5])
    y = grid_search.predict(texts_test)
    print(classification_report(labels_test, y))
    print("confusion matrix:")
    print(confusion_matrix(labels_test, y))
    print(pd.DataFrame({'Predicted': y, 'Expected': labels_test}))

    print("Test results on data sampled only from buzzfeedTop (mixed claims):")
    texts_test, labels_test, texts, labels = DataLoading.balance_data(texts_buzzfeedTop, labels_buzzfeedTop ,sample_size=None , discard_labels=[])
    y = grid_search.predict(texts_test)
    print(classification_report(labels_test, y))
    print("confusion matrix:")
    print(confusion_matrix(labels_test, y))
    print(pd.DataFrame({'Predicted': y, 'Expected': labels_test}))

    print("Test results on data sampled only from perez (celebrity stories):")
    texts_test, labels_test, texts, labels = DataLoading.balance_data(texts_perez, labels_perez ,sample_size=None , discard_labels=[])
    y = grid_search.predict(texts_test)
    print(classification_report(labels_test, y))
    print("confusion matrix:")
    print(confusion_matrix(labels_test, y))
    print(pd.DataFrame({'Predicted': y, 'Expected': labels_test}))









/Users/fa/anaconda/envs/py35/bin/python /Users/fa/workspace/shared/sfu/fake_news/src/union_classification_explore.py
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.
  _nan_object_mask = _nan_object_array != _nan_object_array
Preparing lexicons & lwicDic
Number of terms in the lexicon act_adverbs.txt : 15
Number of terms in the lexicon assertives_hooper1975.txt : 67
Number of terms in the lexicon comparative_forms.txt : 2122
Number of terms in the lexicon factives_hooper1975.txt : 29
Number of terms in the lexicon hedges_hyland2005.txt : 105
Number of terms in the lexicon implicatives_karttunen1971.txt : 32
Number of terms in the lexicon manner_adverbs.txt : 128
Number of terms in the lexicon modal_adverbs.txt : 94
Number of terms in the lexicon negative-HuLui.txt : 4784
Number of terms in the lexicon negative_mpqa.txt : 3078
Number of terms in the lexicon neutral_mpqa.txt : 175
Number of terms in the lexicon posative_mpqa.txt : 2304
Number of terms in the lexicon positive-HuLui.txt : 2007
Number of terms in the lexicon report_verbs.txt : 181
Number of terms in the lexicon superlative_forms.txt : 2306
Data loaded from disk!
Size of train, validataion and test sets: 646 , 0 , 0
0    Clinton gets it wrong on 'small business' job ...
1     Uber launched a fleet of its much anticipated...
2    Story highlights US State Department commends ...
dtype: object
0    0
1    0
2    0
dtype: int64
Loading data snopes...
(118, 5)
0    mixture
1     mfalse
2    mixture
3    mixture
4      mtrue
5     ffalse
6     mfalse
7      mtrue
8     ffalse
9     ffalse
Name: label, dtype: object
['mixture' 'mfalse' 'mtrue' 'ffalse' 'ftrue']
Data from Snopes looks like...
0     Massive Pedophile Ring With '70,000 Elite Mem...
1     NBC Bay Area's SkyRanger on Saturday captured...
2     Today is Flag Day, the anniversary of when 19...
3     HONOLULU Federal authorities on Friday added ...
4     Text smaller    Text bigger    We have all he...
5     A 79-year-old retired officer of the CIA, Bil...
6     Governor Jerry Brown is retiring but not befo...
7     Sneed: 108 could be the Cubs magic number thi...
8     Clint Eastwood, more famous for westerns than...
9     Delaware City Council passed a resolution thi...
Name: data, dtype: object
[5, 1, 5, 5, 0, 1, 1, 0, 1, 1]
0    48
1    44
5    26
dtype: int64
Loading data buzzfeedtop...
(33, 15)
                                               title  \
0  Babysitter transported to hospital after inser...
1  FBI seizes over 3,000 penises during raid at m...
2  Charles Manson to be released on parole, to Jo...

                                                 url Politifact  \
0  http://worldnewsdailyreport.com/babysitter-tra...        NaN
1  http://worldnewsdailyreport.com/fbi-seizes-ove...        NaN
2  http://www.breakingnews365.net/59690fb994b9c/c...        NaN

   Politifact FB                                             Snopes  \
0            NaN  https://www.snopes.com/babysitter-transported-...
1            NaN  https://www.snopes.com/fbi-seizes-3000-penises...
2            NaN  https://www.snopes.com/politics/satire/mansonp...

   Snopes FB Factcheck  Factcheck FB  ABC  ABC FB error_phase2  \
0     1734.0       NaN           NaN  NaN     NaN     No Error
1       14.0       NaN           NaN  NaN     NaN     No Error
2        NaN       NaN           NaN  NaN     NaN     No Error

                        original_article_text_phase2  \
0  Cincinnati, Ohio | A 31-year old woman was adm...
1  FBI agents made an astonishing discovery this ...
2  Giant Squid Washes Ashore on Lake Michigan Mic...

                                article_title_phase2 publish_date_phase2  \
0  Babysitter transported to hospital after inser...          2017-05-03
1  FBI seizes over 3,000 penises during raid at m...          2017-09-25
2  Charles Manson to be released on parole, to Jo...                 NaN

       author_phase2
0  Barbara Jennnings
1  Barbara Jennnings
2                NaN
Data from BuzzFeed looks like...
0    Cincinnati, Ohio | A 31-year old woman was adm...
1    FBI agents made an astonishing discovery this ...
2    Giant Squid Washes Ashore on Lake Michigan Mic...
3    Police have reportedly launched a murder inves...
4    Beaumont, Texas | An employee of the Jefferson...
5    WASHINGTON, DC (By J. McConkey)A group of lead...
6    WASHINGTON, D.C.  In another sweeping move aim...
7    Columbus, Ohio | An 83-year old woman was arre...
8    A couple was transported to the hospital in a ...
9    Darrel Whitaker from Glenwood Springs in Color...
Name: original_article_text_phase2, dtype: object
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
1    33
dtype: int64
Loading data perez...
(500, 3)
   Unnamed: 0                                               text  label
0           0  Jennifer Aniston dashes 'Friends' reunion hope...  legit
1           1  This Is What Brad Pitt Has Been Texting Jennif...  legit
2           2  Jennifer Aniston's spokesman denies reports th...  legit
Data from perez looks like...
0    Jennifer Aniston dashes 'Friends' reunion hope...
1    This Is What Brad Pitt Has Been Texting Jennif...
2    Jennifer Aniston's spokesman denies reports th...
3    Jennifer Aniston sparks adoption rumors\n\nBef...
4    Jennifer Aniston denies she had an affair with...
5    Jennifer Aniston: I'm Not a 'Sad, Childless Hu...
6    Jennifer Aniston Finally Pregnant At 48 Years ...
7    Brad Pitt is not reuniting with Jennifer Anist...
8    Miley And Liam Fighting? False Rumors Swirl Th...
9    Kristen Stewart not dropping another "Twilight...
Name: text, dtype: object
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
1    250
0    250
dtype: int64

*** Feature weigths ***
 {'lexicon_features': 0.2, 'surface_features': 0.0, 'liwc_features': 0.2, 'readability_features': 0.0, 'pos_features': 0.0, 'body_bow': 0.2}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'train_score': array([ 0.96976744,  0.96744186,  0.96064815]), 'score_time': array([ 26.20594501,  27.38972402,  30.20805907]), 'fit_time': array([ 60.72420597,  56.71235299,  53.41128802]), 'test_score': array([ 0.58796296,  0.60648148,  0.61214953])}
 Mean train score:  0.965952483491
 Mean test score:  0.602197992385
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.93      0.97      0.95       323
           1       0.97      0.93      0.95       323

   micro avg       0.95      0.95      0.95       646
   macro avg       0.95      0.95      0.95       646
weighted avg       0.95      0.95      0.95       646

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.69      0.75      0.72        48
           1       0.70      0.64      0.67        44

   micro avg       0.70      0.70      0.70        92
   macro avg       0.70      0.69      0.69        92
weighted avg       0.70      0.70      0.69        92

confusion matrix:
[[36 12]
 [16 28]]
    Expected  Predicted
0          0          0
1          0          0
2          0          1
3          0          0
4          0          0
5          0          0
6          0          0
7          0          0
8          0          0
9          0          1
10         0          0
11         0          0
12         0          0
13         0          0
14         0          0
15         0          0
16         0          0
17         0          0
18         0          0
19         0          0
20         0          1
21         0          1
22         0          0
23         0          0
24         0          0
25         0          0
26         0          0
27         0          1
28         0          1
29         0          0
..       ...        ...
62         1          1
63         1          1
64         1          1
65         1          1
66         1          1
67         1          1
68         1          0
69         1          1
70         1          1
71         1          0
72         1          0
73         1          1
74         1          1
75         1          1
76         1          0
77         1          1
78         1          0
79         1          1
80         1          1
81         1          1
82         1          1
83         1          0
84         1          1
85         1          1
86         1          0
87         1          1
88         1          1
89         1          1
90         1          1
91         1          0

[92 rows x 2 columns]
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.88      0.94        33

/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
   micro avg       0.88      0.88      0.88        33
  'recall', 'true', average, warn_for)
   macro avg       0.50      0.44      0.47        33
weighted avg       1.00      0.88      0.94        33

confusion matrix:
[[ 0  0]
 [ 4 29]]
    Expected  Predicted
0          1          1
1          1          1
2          1          1
3          1          1
4          1          1
5          1          1
6          1          1
7          1          1
8          1          1
9          1          1
10         1          1
11         1          1
12         1          1
13         1          1
14         1          1
15         1          1
16         1          1
17         1          1
18         1          1
19         1          0
20         1          0
21         1          1
22         1          1
23         1          1
24         1          1
25         1          1
26         1          1
27         1          1
28         1          0
29         1          1
30         1          1
31         1          1
32         1          0
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.67      0.56      0.61       250
           1       0.62      0.72      0.67       250

   micro avg       0.64      0.64      0.64       500
   macro avg       0.65      0.64      0.64       500
weighted avg       0.65      0.64      0.64       500

confusion matrix:
[[141 109]
 [ 70 180]]
     Expected  Predicted
0           0          1
1           0          1
2           0          1
3           0          1
4           0          1
5           0          1
6           0          1
7           0          1
8           0          1
9           0          1
10          0          0
11          0          0
12          0          1
13          0          0
14          0          1
15          0          1
16          0          1
17          0          1
18          0          1
19          0          0
20          0          0
21          0          0
22          0          1
23          0          0
24          0          1
25          0          1
26          0          0
27          0          0
28          0          1
29          0          1
..        ...        ...
470         1          1
471         1          1
472         1          1
473         1          1
474         1          0
475         1          0
476         1          1
477         1          1
478         1          0
479         1          1
480         1          1
481         1          1
482         1          0
483         1          0
484         1          0
485         1          1
486         1          1
487         1          1
488         1          1
489         1          1
490         1          1
491         1          1
492         1          0
493         1          1
494         1          0
495         1          1
496         1          1
497         1          1
498         1          1
499         1          1

[500 rows x 2 columns]

*** Feature weigths ***
 {'lexicon_features': 0.2, 'surface_features': 0.2, 'liwc_features': 0.2, 'readability_features': 0.2, 'pos_features': 0.2, 'body_bow': 0.2}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'train_score': array([ 0.79302326,  0.60697674,  0.63425926]), 'score_time': array([ 24.16286016,  25.78287506,  28.44366097]), 'fit_time': array([ 54.84788799,  53.66812396,  50.30396891]), 'test_score': array([ 0.49537037,  0.56018519,  0.53271028])}
 Mean train score:  0.678086419753
 Mean test score:  0.52942194531
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.86      0.86      0.86       323
           1       0.86      0.85      0.86       323

   micro avg       0.86      0.86      0.86       646
   macro avg       0.86      0.86      0.86       646
weighted avg       0.86      0.86      0.86       646

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.62      0.52      0.57        48
           1       0.56      0.66      0.60        44

   micro avg       0.59      0.59      0.59        92
   macro avg       0.59      0.59      0.59        92
weighted avg       0.59      0.59      0.59        92

confusion matrix:
[[25 23]
 [15 29]]
    Expected  Predicted
0          0          1
1          0          0
2          0          1
3          0          0
4          0          0
5          0          1
6          0          1
7          0          0
8          0          0
9          0          1
10         0          1
11         0          1
12         0          0
13         0          1
14         0          0
15         0          0
16         0          0
17         0          0
18         0          1
19         0          0
20         0          1
21         0          1
22         0          0
23         0          0
24         0          0
25         0          0
26         0          0
27         0          1
28         0          0
29         0          0
..       ...        ...
62         1          1
63         1          1
64         1          1
65         1          1
66         1          1
67         1          1
68         1          0
69         1          1
70         1          1
71         1          0
72         1          1
73         1          1
74         1          1
75         1          1
76         1          0
77         1          1
78         1          0
79         1          1
80         1          1
81         1          1
82         1          0
83         1          0
84         1          1
85         1          1
86         1          0
87         1          1
88         1          0
89         1          1
90         1          1
91         1          0

[92 rows x 2 columns]
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.97      0.98        33

   micro avg       0.97      0.97      0.97        33
   macro avg       0.50      0.48      0.49        33
weighted avg       1.00      0.97      0.98        33

confusion matrix:
[[ 0  0]
 [ 1 32]]
    Expected  Predicted
0          1          1
1          1          1
2          1          1
3          1          1
4          1          1
5          1          1
6          1          1
7          1          1
8          1          1
9          1          1
10         1          1
11         1          1
12         1          1
13         1          1
14         1          1
15         1          1
16         1          1
17         1          1
18         1          1
19         1          1
20         1          1
21         1          1
22         1          1
23         1          1
24         1          1
25         1          1
26         1          1
27         1          1
28         1          0
29         1          1
30         1          1
31         1          1
32         1          1
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
              precision    recall  f1-score   support

           0       0.56      0.78      0.65       250
           1       0.64      0.38      0.47       250

   micro avg       0.58      0.58      0.58       500
   macro avg       0.60      0.58      0.56       500
weighted avg       0.60      0.58      0.56       500

confusion matrix:
[[196  54]
 [156  94]]
     Expected  Predicted
0           0          0
1           0          0
2           0          0
3           0          1
4           0          0
5           0          1
6           0          1
7           0          0
8           0          0
9           0          1
10          0          0
11          0          0
12          0          1
13          0          0
14          0          1
15          0          0
16          0          0
17          0          1
18          0          0
19          0          0
20          0          0
21          0          0
22          0          0
23          0          0
24          0          0
25          0          0
26          0          0
27          0          0
28          0          0
29          0          1
..        ...        ...
470         1          0
471         1          0
472         1          0
473         1          0
474         1          0
475         1          0
476         1          0
477         1          0
478         1          0
479         1          0
480         1          1
481         1          1
482         1          0
483         1          0
484         1          0
485         1          1
486         1          1
487         1          0
488         1          1
489         1          0
490         1          1
491         1          1
492         1          0
493         1          1
494         1          0
495         1          0
496         1          0
497         1          0
498         1          1
499         1          0

[500 rows x 2 columns]

*** Feature weigths ***
 {'lexicon_features': 0.0, 'surface_features': 0.0, 'liwc_features': 0.0, 'readability_features': 0.0, 'pos_features': 0.0, 'body_bow': 0.2}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'train_score': array([ 0.9744186 ,  0.9744186 ,  0.96759259]), 'score_time': array([ 24.17707992,  25.76328802,  28.513165  ]), 'fit_time': array([ 54.74466801,  53.411695  ,  50.42856789]), 'test_score': array([ 0.59259259,  0.59722222,  0.62616822])}
 Mean train score:  0.972143267298
 Mean test score:  0.605327679705
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.95      0.98      0.96       323
           1       0.97      0.95      0.96       323

   micro avg       0.96      0.96      0.96       646
   macro avg       0.96      0.96      0.96       646
weighted avg       0.96      0.96      0.96       646

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.68      0.71      0.69        48
           1       0.67      0.64      0.65        44

   micro avg       0.67      0.67      0.67        92
   macro avg       0.67      0.67      0.67        92
weighted avg       0.67      0.67      0.67        92

confusion matrix:
[[34 14]
 [16 28]]
    Expected  Predicted
0          0          0
1          0          0
2          0          1
3          0          0
4          0          0
5          0          0
6          0          0
7          0          0
8          0          0
9          0          1
10         0          0
11         0          0
12         0          0
13         0          0
14         0          0
15         0          0
16         0          0
17         0          0
18         0          0
19         0          0
20         0          1
21         0          1
22         0          1
23         0          0
24         0          0
25         0          0
26         0          0
27         0          1
28         0          1
29         0          0
..       ...        ...
62         1          1
63         1          1
64         1          1
65         1          1
66         1          1
67         1          1
68         1          0
69         1          1
70         1          1
71         1          0
72         1          0
73         1          1
74         1          1
75         1          1
76         1          0
77         1          1
78         1          0
79         1          1
80         1          1
81         1          1
82         1          1
83         1          0
84         1          1
85         1          1
86         1          0
87         1          1
88         1          1
89         1          1
90         1          1
91         1          0

[92 rows x 2 columns]
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.88      0.94        33

   micro avg       0.88      0.88      0.88        33
   macro avg       0.50      0.44      0.47        33
weighted avg       1.00      0.88      0.94        33

confusion matrix:
[[ 0  0]
 [ 4 29]]
    Expected  Predicted
0          1          1
1          1          1
2          1          1
3          1          1
4          1          1
5          1          1
6          1          1
7          1          1
8          1          1
9          1          1
10         1          1
11         1          1
12         1          1
13         1          1
14         1          1
15         1          1
16         1          1
17         1          1
18         1          1
19         1          0
20         1          0
21         1          1
22         1          1
23         1          1
24         1          1
25         1          1
26         1          1
27         1          1
28         1          0
29         1          1
30         1          1
31         1          1
32         1          0
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.67      0.55      0.60       250
           1       0.62      0.73      0.67       250

   micro avg       0.64      0.64      0.64       500
   macro avg       0.64      0.64      0.64       500
weighted avg       0.64      0.64      0.64       500

confusion matrix:
[[137 113]
 [ 67 183]]
     Expected  Predicted
0           0          0
1           0          1
2           0          1
3           0          1
4           0          1
5           0          1
6           0          1
7           0          1
8           0          1
9           0          1
10          0          0
11          0          1
12          0          1
13          0          1
14          0          1
15          0          1
16          0          1
17          0          1
18          0          1
19          0          0
20          0          0
21          0          0
22          0          0
23          0          0
24          0          1
25          0          0
26          0          0
27          0          0
28          0          1
29          0          1
..        ...        ...
470         1          1
471         1          1
472         1          1
473         1          1
474         1          0
475         1          0
476         1          1
477         1          1
478         1          1
479         1          1
480         1          1
481         1          1
482         1          1
483         1          0
484         1          0
485         1          1
486         1          1
487         1          1
488         1          1
489         1          1
490         1          1
491         1          1
492         1          0
493         1          1
494         1          0
495         1          1
496         1          1
497         1          1
498         1          1
499         1          1

[500 rows x 2 columns]

*** Feature weigths ***
 {'lexicon_features': 0.0, 'surface_features': 0.2, 'liwc_features': 0.0, 'readability_features': 0.0, 'pos_features': 0.0, 'body_bow': 0.0}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'train_score': array([ 0.55116279,  0.51860465,  0.54398148]), 'score_time': array([ 26.01040077,  27.6971941 ,  30.78655791]), 'fit_time': array([ 58.94750714,  57.59856081,  54.28026509]), 'test_score': array([ 0.47685185,  0.55092593,  0.44392523])}
 Mean train score:  0.537916307781
 Mean test score:  0.490567670474
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.57      0.45      0.50       323
           1       0.54      0.66      0.60       323

   micro avg       0.55      0.55      0.55       646
   macro avg       0.56      0.55      0.55       646
weighted avg       0.56      0.55      0.55       646

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.58      0.38      0.46        48
           1       0.51      0.70      0.59        44

   micro avg       0.53      0.53      0.53        92
   macro avg       0.54      0.54      0.52        92
weighted avg       0.55      0.53      0.52        92

confusion matrix:
[[18 30]
 [13 31]]
    Expected  Predicted
0          0          0
1          0          0
2          0          1
3          0          1
4          0          0
5          0          1
6          0          1
7          0          1
8          0          0
9          0          1
10         0          1
11         0          0
12         0          1
13         0          1
14         0          0
15         0          1
16         0          1
17         0          1
18         0          0
19         0          0
20         0          0
21         0          0
22         0          0
23         0          1
24         0          0
25         0          0
26         0          1
27         0          0
28         0          0
29         0          1
..       ...        ...
62         1          0
63         1          1
64         1          1
65         1          1
66         1          1
67         1          0
68         1          1
69         1          1
70         1          1
71         1          1
72         1          1
73         1          1
74         1          1
75         1          1
76         1          0
77         1          1
78         1          1
79         1          1
80         1          1
81         1          1
82         1          1
83         1          1
84         1          1
85         1          1
86         1          0
87         1          0
88         1          0
89         1          1
90         1          1
91         1          0

[92 rows x 2 columns]
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.88      0.94        33

   micro avg       0.88      0.88      0.88        33
   macro avg       0.50      0.44      0.47        33
weighted avg       1.00      0.88      0.94        33

confusion matrix:
[[ 0  0]
 [ 4 29]]
    Expected  Predicted
0          1          1
1          1          1
2          1          1
3          1          0
4          1          1
5          1          1
6          1          1
7          1          1
8          1          1
9          1          1
10         1          1
11         1          1
12         1          0
13         1          1
14         1          1
15         1          1
16         1          1
17         1          1
18         1          1
19         1          0
20         1          1
21         1          1
22         1          1
23         1          1
24         1          1
25         1          0
26         1          1
27         1          1
28         1          1
29         1          1
30         1          1
31         1          1
32         1          1
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.70      0.21      0.32       250
           1       0.54      0.91      0.67       250

   micro avg       0.56      0.56      0.56       500
   macro avg       0.62      0.56      0.50       500
weighted avg       0.62      0.56      0.50       500

confusion matrix:
[[ 52 198]
 [ 22 228]]
     Expected  Predicted
0           0          1
1           0          1
2           0          1
3           0          1
4           0          0
5           0          1
6           0          1
7           0          1
8           0          0
9           0          1
10          0          1
11          0          1
12          0          1
13          0          0
14          0          1
15          0          1
16          0          1
17          0          1
18          0          1
19          0          1
20          0          0
21          0          1
22          0          1
23          0          1
24          0          1
25          0          0
26          0          1
27          0          1
28          0          1
29          0          1
..        ...        ...
470         1          1
471         1          1
472         1          1
473         1          1
474         1          1
475         1          1
476         1          1
477         1          1
478         1          1
479         1          1
480         1          1
481         1          1
482         1          1
483         1          1
484         1          0
485         1          1
486         1          1
487         1          1
488         1          1
489         1          1
490         1          1
491         1          1
492         1          1
493         1          1
494         1          1
495         1          1
496         1          1
497         1          1
498         1          1
499         1          1

[500 rows x 2 columns]

*** Feature weigths ***
 {'lexicon_features': 0.2, 'surface_features': 0.0, 'liwc_features': 0.0, 'readability_features': 0.0, 'pos_features': 0.0, 'body_bow': 0.0}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'train_score': array([ 0.58837209,  0.56744186,  0.56944444]), 'score_time': array([ 26.65418792,  27.72116208,  30.70641494]), 'fit_time': array([ 61.0606029 ,  57.48966694,  54.22388816]), 'test_score': array([ 0.52314815,  0.56944444,  0.56542056])}
 Mean train score:  0.575086132644
 Mean test score:  0.552671051113
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.57      0.54      0.56       323
           1       0.56      0.59      0.58       323

   micro avg       0.57      0.57      0.57       646
   macro avg       0.57      0.57      0.57       646
weighted avg       0.57      0.57      0.57       646

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.57      0.52      0.54        48
           1       0.52      0.57      0.54        44

   micro avg       0.54      0.54      0.54        92
   macro avg       0.54      0.54      0.54        92
weighted avg       0.55      0.54      0.54        92

confusion matrix:
[[25 23]
 [19 25]]
    Expected  Predicted
0          0          1
1          0          0
2          0          0
3          0          1
4          0          1
5          0          0
6          0          1
7          0          0
8          0          0
9          0          0
10         0          0
11         0          0
12         0          1
13         0          1
14         0          1
15         0          0
16         0          0
17         0          0
18         0          1
19         0          0
20         0          0
21         0          1
22         0          1
23         0          1
24         0          1
25         0          0
26         0          0
27         0          1
28         0          1
29         0          1
..       ...        ...
62         1          0
63         1          1
64         1          0
65         1          1
66         1          1
67         1          1
68         1          1
69         1          0
70         1          0
71         1          1
72         1          1
73         1          1
74         1          1
75         1          1
76         1          1
77         1          1
78         1          0
79         1          1
80         1          1
81         1          1
82         1          0
83         1          0
84         1          1
85         1          1
86         1          0
87         1          1
88         1          0
89         1          1
90         1          1
91         1          0

[92 rows x 2 columns]
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.82      0.90        33

   micro avg       0.82      0.82      0.82        33
   macro avg       0.50      0.41      0.45        33
weighted avg       1.00      0.82      0.90        33

confusion matrix:
[[ 0  0]
 [ 6 27]]
    Expected  Predicted
0          1          1
1          1          1
2          1          0
3          1          1
4          1          1
5          1          1
6          1          1
7          1          1
8          1          1
9          1          1
10         1          1
11         1          1
12         1          1
13         1          1
14         1          1
15         1          1
16         1          0
17         1          1
18         1          1
19         1          1
20         1          1
21         1          0
22         1          1
23         1          1
24         1          0
25         1          1
26         1          0
27         1          1
28         1          1
29         1          0
30         1          1
31         1          1
32         1          1
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.56      0.84      0.67       250
           1       0.68      0.36      0.47       250

   micro avg       0.60      0.60      0.60       500
   macro avg       0.62      0.60      0.57       500
weighted avg       0.62      0.60      0.57       500

confusion matrix:
[[209  41]
 [161  89]]
     Expected  Predicted
0           0          0
1           0          0
2           0          1
3           0          1
4           0          0
5           0          1
6           0          1
7           0          1
8           0          0
9           0          0
10          0          0
11          0          0
12          0          0
13          0          0
14          0          0
15          0          0
16          0          0
17          0          0
18          0          0
19          0          0
20          0          0
21          0          0
22          0          0
23          0          0
24          0          0
25          0          0
26          0          0
27          0          1
28          0          1
29          0          1
..        ...        ...
470         1          1
471         1          0
472         1          0
473         1          0
474         1          1
475         1          1
476         1          0
477         1          0
478         1          0
479         1          1
480         1          0
481         1          0
482         1          0
483         1          0
484         1          0
485         1          0
486         1          0
487         1          1
488         1          1
489         1          1
490         1          0
491         1          1
492         1          0
493         1          0
494         1          1
495         1          0
496         1          1
497         1          0
498         1          1
499         1          0

[500 rows x 2 columns]

*** Feature weigths ***
 {'lexicon_features': 0.0, 'surface_features': 0.0, 'liwc_features': 0.2, 'readability_features': 0.0, 'pos_features': 0.0, 'body_bow': 0.0}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'train_score': array([ 0.5744186 ,  0.56744186,  0.58333333]), 'score_time': array([ 26.06663609,  25.65863395,  28.672333  ]), 'fit_time': array([ 59.26108384,  53.44280005,  50.36377001]), 'test_score': array([ 0.56944444,  0.58796296,  0.53271028])}
 Mean train score:  0.575064599483
 Mean test score:  0.563372562594
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.58      0.58      0.58       323
           1       0.58      0.58      0.58       323

   micro avg       0.58      0.58      0.58       646
   macro avg       0.58      0.58      0.58       646
weighted avg       0.58      0.58      0.58       646

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.54      0.54      0.54        48
           1       0.50      0.50      0.50        44

   micro avg       0.52      0.52      0.52        92
   macro avg       0.52      0.52      0.52        92
weighted avg       0.52      0.52      0.52        92

confusion matrix:
[[26 22]
 [22 22]]
    Expected  Predicted
0          0          1
1          0          0
2          0          0
3          0          0
4          0          0
5          0          0
6          0          1
7          0          0
8          0          0
9          0          1
10         0          1
11         0          0
12         0          0
13         0          1
14         0          1
15         0          1
16         0          0
17         0          0
18         0          0
19         0          0
20         0          0
21         0          1
22         0          0
23         0          1
24         0          0
25         0          1
26         0          1
27         0          1
28         0          0
29         0          1
..       ...        ...
62         1          1
63         1          1
64         1          1
65         1          0
66         1          0
67         1          0
68         1          1
69         1          0
70         1          1
71         1          1
72         1          1
73         1          0
74         1          0
75         1          1
76         1          1
77         1          1
78         1          1
79         1          1
80         1          0
81         1          1
82         1          1
83         1          0
84         1          0
85         1          1
86         1          0
87         1          1
88         1          0
89         1          0
90         1          1
91         1          0

[92 rows x 2 columns]
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.70      0.82        33

   micro avg       0.70      0.70      0.70        33
   macro avg       0.50      0.35      0.41        33
weighted avg       1.00      0.70      0.82        33

confusion matrix:
[[ 0  0]
 [10 23]]
    Expected  Predicted
0          1          1
1          1          1
2          1          1
3          1          1
4          1          1
5          1          1
6          1          0
7          1          0
8          1          1
9          1          1
10         1          0
11         1          1
12         1          1
13         1          1
14         1          1
15         1          1
16         1          0
17         1          0
18         1          1
19         1          1
20         1          1
21         1          1
22         1          0
23         1          1
24         1          1
25         1          1
26         1          0
27         1          1
28         1          0
29         1          0
30         1          1
31         1          1
32         1          0
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
              precision    recall  f1-score   support

           0       0.52      0.49      0.51       250
           1       0.52      0.54      0.53       250

   micro avg       0.52      0.52      0.52       500
   macro avg       0.52      0.52      0.52       500
weighted avg       0.52      0.52      0.52       500

confusion matrix:
[[123 127]
 [114 136]]
     Expected  Predicted
0           0          1
1           0          1
2           0          0
3           0          1
4           0          1
5           0          1
6           0          1
7           0          1
8           0          1
9           0          1
10          0          0
11          0          0
12          0          1
13          0          0
14          0          1
15          0          0
16          0          1
17          0          1
18          0          0
19          0          0
20          0          1
21          0          0
22          0          1
23          0          0
24          0          1
25          0          1
26          0          1
27          0          0
28          0          0
29          0          1
..        ...        ...
470         1          1
471         1          0
472         1          1
473         1          0
474         1          1
475         1          1
476         1          0
477         1          0
478         1          0
479         1          0
480         1          0
481         1          0
482         1          1
483         1          1
484         1          1
485         1          1
486         1          1
487         1          1
488         1          0
489         1          0
490         1          1
491         1          1
492         1          1
493         1          1
494         1          0
495         1          0
496         1          0
497         1          0
498         1          1
499         1          0

[500 rows x 2 columns]

*** Feature weigths ***
 {'lexicon_features': 0.0, 'surface_features': 0.0, 'liwc_features': 0.0, 'readability_features': 0.0, 'pos_features': 0.2, 'body_bow': 0.0}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'train_score': array([ 0.57674419,  0.57906977,  0.60185185]), 'score_time': array([ 24.24080992,  25.78251791,  28.5854218 ]), 'fit_time': array([ 54.73322606,  53.34927702,  50.48762512]), 'test_score': array([ 0.61574074,  0.5787037 ,  0.53738318])}
 Mean train score:  0.58588860178
 Mean test score:  0.577275874005
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.60      0.54      0.57       323
           1       0.58      0.64      0.61       323

   micro avg       0.59      0.59      0.59       646
   macro avg       0.59      0.59      0.59       646
weighted avg       0.59      0.59      0.59       646

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.51      0.42      0.46        48
           1       0.47      0.57      0.52        44

   micro avg       0.49      0.49      0.49        92
   macro avg       0.49      0.49      0.49        92
weighted avg       0.49      0.49      0.49        92

confusion matrix:
[[20 28]
 [19 25]]
    Expected  Predicted
0          0          1
1          0          0
2          0          0
3          0          1
4          0          1
5          0          0
6          0          1
7          0          0
8          0          0
9          0          1
10         0          1
11         0          0
12         0          0
13         0          1
14         0          1
15         0          1
16         0          0
17         0          0
18         0          1
19         0          1
20         0          0
21         0          1
22         0          0
23         0          1
24         0          0
25         0          0
26         0          1
27         0          1
28         0          0
29         0          1
..       ...        ...
62         1          1
63         1          1
64         1          1
65         1          0
66         1          0
67         1          0
68         1          1
69         1          0
70         1          1
71         1          0
72         1          1
73         1          1
74         1          0
75         1          1
76         1          1
77         1          1
78         1          1
79         1          1
80         1          0
81         1          1
82         1          1
83         1          0
84         1          1
85         1          1
86         1          0
87         1          1
88         1          0
89         1          1
90         1          1
91         1          1

[92 rows x 2 columns]
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.82      0.90        33

   micro avg       0.82      0.82      0.82        33
   macro avg       0.50      0.41      0.45        33
weighted avg       1.00      0.82      0.90        33

confusion matrix:
[[ 0  0]
 [ 6 27]]
    Expected  Predicted
0          1          1
1          1          1
2          1          1
3          1          1
4          1          1
5          1          1
6          1          1
7          1          0
8          1          1
9          1          1
10         1          0
11         1          1
12         1          1
13         1          1
14         1          1
15         1          1
16         1          0
17         1          1
18         1          1
19         1          1
20         1          1
21         1          1
22         1          1
23         1          1
24         1          1
25         1          1
26         1          0
27         1          1
28         1          0
29         1          0
30         1          1
31         1          1
32         1          1
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
              precision    recall  f1-score   support

           0       0.55      0.45      0.49       250
           1       0.53      0.63      0.58       250

   micro avg       0.54      0.54      0.54       500
   macro avg       0.54      0.54      0.54       500
weighted avg       0.54      0.54      0.54       500

confusion matrix:
[[112 138]
 [ 92 158]]
     Expected  Predicted
0           0          1
1           0          1
2           0          1
3           0          1
4           0          1
5           0          1
6           0          1
7           0          0
8           0          0
9           0          1
10          0          1
11          0          0
12          0          1
13          0          0
14          0          0
15          0          1
16          0          1
17          0          1
18          0          0
19          0          0
20          0          1
21          0          1
22          0          1
23          0          0
24          0          1
25          0          1
26          0          1
27          0          0
28          0          0
29          0          1
..        ...        ...
470         1          1
471         1          0
472         1          1
473         1          0
474         1          1
475         1          1
476         1          0
477         1          0
478         1          0
479         1          0
480         1          0
481         1          1
482         1          1
483         1          1
484         1          1
485         1          1
486         1          1
487         1          1
488         1          0
489         1          1
490         1          1
491         1          1
492         1          1
493         1          1
494         1          1
495         1          0
496         1          0
497         1          1
498         1          1
499         1          0

[500 rows x 2 columns]

*** Feature weigths ***
 {'lexicon_features': 0.0, 'surface_features': 0.0, 'liwc_features': 0.0, 'readability_features': 0.2, 'pos_features': 0.0, 'body_bow': 0.0}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'train_score': array([ 0.53953488,  0.54883721,  0.56018519]), 'score_time': array([ 24.19382501,  25.8042841 ,  28.55065608]), 'fit_time': array([ 54.78362203,  53.36727881,  50.4664259 ]), 'test_score': array([ 0.43518519,  0.51388889,  0.54672897])}
 Mean train score:  0.549519092736
 Mean test score:  0.498601015346
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.54      0.50      0.52       323
           1       0.53      0.56      0.55       323

   micro avg       0.53      0.53      0.53       646
   macro avg       0.53      0.53      0.53       646
weighted avg       0.53      0.53      0.53       646

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.49      0.35      0.41        48
           1       0.46      0.59      0.51        44

   micro avg       0.47      0.47      0.47        92
   macro avg       0.47      0.47      0.46        92
weighted avg       0.47      0.47      0.46        92

confusion matrix:
[[17 31]
 [18 26]]
    Expected  Predicted
0          0          1
1          0          1
2          0          0
3          0          1
4          0          0
5          0          1
6          0          1
7          0          1
8          0          0
9          0          1
10         0          1
11         0          1
12         0          0
13         0          1
14         0          0
15         0          1
16         0          1
17         0          0
18         0          1
19         0          1
20         0          0
21         0          0
22         0          0
23         0          1
24         0          0
25         0          0
26         0          0
27         0          1
28         0          0
29         0          1
..       ...        ...
62         1          1
63         1          1
64         1          1
65         1          0
66         1          1
67         1          1
68         1          0
69         1          0
70         1          1
71         1          0
72         1          1
73         1          0
74         1          1
75         1          0
76         1          1
77         1          1
78         1          0
79         1          1
80         1          1
81         1          1
82         1          1
83         1          0
84         1          1
85         1          1
86         1          1
87         1          0
88         1          0
89         1          0
90         1          1
91         1          0

[92 rows x 2 columns]
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.76      0.86        33

   micro avg       0.76      0.76      0.76        33
   macro avg       0.50      0.38      0.43        33
weighted avg       1.00      0.76      0.86        33

confusion matrix:
[[ 0  0]
 [ 8 25]]
    Expected  Predicted
0          1          1
1          1          1
2          1          1
3          1          0
4          1          1
5          1          1
6          1          0
7          1          0
8          1          1
9          1          1
10         1          0
11         1          1
12         1          0
13         1          0
14         1          0
15         1          1
16         1          1
17         1          1
18         1          1
19         1          1
20         1          1
21         1          1
22         1          1
23         1          1
24         1          1
25         1          1
26         1          1
27         1          1
28         1          0
29         1          1
30         1          1
31         1          1
32         1          1
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
              precision    recall  f1-score   support

           0       0.51      0.80      0.62       250
           1       0.52      0.22      0.31       250

   micro avg       0.51      0.51      0.51       500
   macro avg       0.52      0.51      0.47       500
weighted avg       0.52      0.51      0.47       500

confusion matrix:
[[200  50]
 [195  55]]
     Expected  Predicted
0           0          0
1           0          0
2           0          0
3           0          0
4           0          0
5           0          1
6           0          0
7           0          0
8           0          0
9           0          0
10          0          0
11          0          0
12          0          1
13          0          0
14          0          1
15          0          0
16          0          0
17          0          0
18          0          0
19          0          1
20          0          0
21          0          0
22          0          0
23          0          1
24          0          0
25          0          0
26          0          1
27          0          1
28          0          0
29          0          1
..        ...        ...
470         1          0
471         1          0
472         1          0
473         1          0
474         1          0
475         1          0
476         1          0
477         1          0
478         1          1
479         1          0
480         1          0
481         1          1
482         1          0
483         1          0
484         1          0
485         1          0
486         1          0
487         1          1
488         1          1
489         1          0
490         1          0
491         1          0
492         1          0
493         1          0
494         1          0
495         1          0
496         1          0
497         1          0
498         1          0
499         1          0

[500 rows x 2 columns]

Process finished with exit code 0



/Users/fa/anaconda/envs/py35/bin/python /Users/fa/workspace/shared/sfu/fake_news/src/union_classification_explore.py
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.
  _nan_object_mask = _nan_object_array != _nan_object_array
Preparing lexicons & lwicDic
Number of terms in the lexicon act_adverbs.txt : 15
Number of terms in the lexicon assertives_hooper1975.txt : 67
Number of terms in the lexicon comparative_forms.txt : 2122
Number of terms in the lexicon factives_hooper1975.txt : 29
Number of terms in the lexicon hedges_hyland2005.txt : 105
Number of terms in the lexicon implicatives_karttunen1971.txt : 32
Number of terms in the lexicon manner_adverbs.txt : 128
Number of terms in the lexicon modal_adverbs.txt : 94
Number of terms in the lexicon negative-HuLui.txt : 4784
Number of terms in the lexicon negative_mpqa.txt : 3078
Number of terms in the lexicon neutral_mpqa.txt : 175
Number of terms in the lexicon posative_mpqa.txt : 2304
Number of terms in the lexicon positive-HuLui.txt : 2007
Number of terms in the lexicon report_verbs.txt : 181
Number of terms in the lexicon superlative_forms.txt : 2306
Data loaded from disk!
Size of train, validataion and test sets: 646 , 0 , 0
0    Clinton gets it wrong on 'small business' job ...
1     Uber launched a fleet of its much anticipated...
2    Story highlights US State Department commends ...
dtype: object
0    0
1    0
2    0
dtype: int64
Loading data snopes...
(118, 5)
0    mixture
1     mfalse
2    mixture
3    mixture
4      mtrue
5     ffalse
6     mfalse
7      mtrue
8     ffalse
9     ffalse
Name: label, dtype: object
['mixture' 'mfalse' 'mtrue' 'ffalse' 'ftrue']
Data from Snopes looks like...
0     Massive Pedophile Ring With '70,000 Elite Mem...
1     NBC Bay Area's SkyRanger on Saturday captured...
2     Today is Flag Day, the anniversary of when 19...
3     HONOLULU Federal authorities on Friday added ...
4     Text smaller    Text bigger    We have all he...
5     A 79-year-old retired officer of the CIA, Bil...
6     Governor Jerry Brown is retiring but not befo...
7     Sneed: 108 could be the Cubs magic number thi...
8     Clint Eastwood, more famous for westerns than...
9     Delaware City Council passed a resolution thi...
Name: data, dtype: object
[5, 1, 5, 5, 0, 1, 1, 0, 1, 1]
0    48
1    44
5    26
dtype: int64
Loading data buzzfeedtop...
(33, 15)
                                               title  \
0  Babysitter transported to hospital after inser...
1  FBI seizes over 3,000 penises during raid at m...
2  Charles Manson to be released on parole, to Jo...

                                                 url Politifact  \
0  http://worldnewsdailyreport.com/babysitter-tra...        NaN
1  http://worldnewsdailyreport.com/fbi-seizes-ove...        NaN
2  http://www.breakingnews365.net/59690fb994b9c/c...        NaN

   Politifact FB                                             Snopes  \
0            NaN  https://www.snopes.com/babysitter-transported-...
1            NaN  https://www.snopes.com/fbi-seizes-3000-penises...
2            NaN  https://www.snopes.com/politics/satire/mansonp...

   Snopes FB Factcheck  Factcheck FB  ABC  ABC FB error_phase2  \
0     1734.0       NaN           NaN  NaN     NaN     No Error
1       14.0       NaN           NaN  NaN     NaN     No Error
2        NaN       NaN           NaN  NaN     NaN     No Error

                        original_article_text_phase2  \
0  Cincinnati, Ohio | A 31-year old woman was adm...
1  FBI agents made an astonishing discovery this ...
2  Giant Squid Washes Ashore on Lake Michigan Mic...

                                article_title_phase2 publish_date_phase2  \
0  Babysitter transported to hospital after inser...          2017-05-03
1  FBI seizes over 3,000 penises during raid at m...          2017-09-25
2  Charles Manson to be released on parole, to Jo...                 NaN

       author_phase2
0  Barbara Jennnings
1  Barbara Jennnings
2                NaN
Data from BuzzFeed looks like...
0    Cincinnati, Ohio | A 31-year old woman was adm...
1    FBI agents made an astonishing discovery this ...
2    Giant Squid Washes Ashore on Lake Michigan Mic...
3    Police have reportedly launched a murder inves...
4    Beaumont, Texas | An employee of the Jefferson...
5    WASHINGTON, DC (By J. McConkey)A group of lead...
6    WASHINGTON, D.C.  In another sweeping move aim...
7    Columbus, Ohio | An 83-year old woman was arre...
8    A couple was transported to the hospital in a ...
9    Darrel Whitaker from Glenwood Springs in Color...
Name: original_article_text_phase2, dtype: object
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
1    33
dtype: int64
Loading data perez...
(500, 3)
   Unnamed: 0                                               text  label
0           0  Jennifer Aniston dashes 'Friends' reunion hope...  legit
1           1  This Is What Brad Pitt Has Been Texting Jennif...  legit
2           2  Jennifer Aniston's spokesman denies reports th...  legit
Data from perez looks like...
0    Jennifer Aniston dashes 'Friends' reunion hope...
1    This Is What Brad Pitt Has Been Texting Jennif...
2    Jennifer Aniston's spokesman denies reports th...
3    Jennifer Aniston sparks adoption rumors\n\nBef...
4    Jennifer Aniston denies she had an affair with...
5    Jennifer Aniston: I'm Not a 'Sad, Childless Hu...
6    Jennifer Aniston Finally Pregnant At 48 Years ...
7    Brad Pitt is not reuniting with Jennifer Anist...
8    Miley And Liam Fighting? False Rumors Swirl Th...
9    Kristen Stewart not dropping another "Twilight...
Name: text, dtype: object
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
1    250
0    250
dtype: int64

*** Feature weigths ***
 {'body_bow': 0.2, 'lexicon_features': 0.2, 'pos_features': 0.2, 'liwc_features': 0.0, 'surface_features': 0.2, 'readability_features': 0.0}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'fit_time': array([ 62.07287097,  57.28343415,  54.20515394]), 'test_score': array([ 0.47222222,  0.55092593,  0.45327103]), 'train_score': array([ 0.55348837,  0.51627907,  0.56712963]), 'score_time': array([ 26.56727791,  27.69850302,  30.80457687])}
 Mean train score:  0.545632357163
 Mean test score:  0.492139725395
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.57      0.44      0.50       323
           1       0.54      0.66      0.59       323

   micro avg       0.55      0.55      0.55       646
   macro avg       0.55      0.55      0.55       646
weighted avg       0.55      0.55      0.55       646

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.58      0.38      0.46        48
           1       0.51      0.70      0.59        44

   micro avg       0.53      0.53      0.53        92
   macro avg       0.54      0.54      0.52        92
weighted avg       0.55      0.53      0.52        92

confusion matrix:
[[18 30]
 [13 31]]
    Expected  Predicted
0          0          0
1          0          0
2          0          1
3          0          1
4          0          0
5          0          1
6          0          1
7          0          1
8          0          0
9          0          1
10         0          1
11         0          0
12         0          1
13         0          1
14         0          0
15         0          1
16         0          1
17         0          1
18         0          0
19         0          0
20         0          0
21         0          0
22         0          0
23         0          1
24         0          0
25         0          0
26         0          1
27         0          0
28         0          0
29         0          1
..       ...        ...
62         1          0
63         1          1
64         1          1
65         1          1
66         1          1
67         1          0
68         1          1
69         1          1
70         1          1
71         1          1
72         1          1
73         1          1
74         1          1
75         1          1
76         1          0
77         1          1
78         1          1
79         1          1
80         1          1
81         1          1
82         1          1
83         1          1
84         1          1
85         1          1
86         1          0
87         1          0
88         1          0
89         1          1
90         1          1
91         1          0

[92 rows x 2 columns]
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.88      0.94        33

   micro avg       0.88      0.88      0.88        33
   macro avg       0.50      0.44      0.47        33
weighted avg       1.00      0.88      0.94        33

confusion matrix:
[[ 0  0]
 [ 4 29]]
    Expected  Predicted
0          1          1
1          1          1
2          1          1
3          1          0
4          1          1
5          1          1
6          1          1
7          1          1
8          1          1
9          1          1
10         1          1
11         1          1
12         1          0
13         1          1
14         1          1
15         1          1
16         1          1
17         1          1
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
18         1          1
  'recall', 'true', average, warn_for)
19         1          0
20         1          1
21         1          1
22         1          1
23         1          1
24         1          1
25         1          0
26         1          1
27         1          1
28         1          1
29         1          1
30         1          1
31         1          1
32         1          1
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.72      0.21      0.32       250
           1       0.54      0.92      0.68       250

   micro avg       0.56      0.56      0.56       500
   macro avg       0.63      0.56      0.50       500
weighted avg       0.63      0.56      0.50       500

confusion matrix:
[[ 52 198]
 [ 20 230]]
     Expected  Predicted
0           0          1
1           0          1
2           0          1
3           0          1
4           0          0
5           0          1
6           0          1
7           0          1
8           0          0
9           0          1
10          0          1
11          0          1
12          0          1
13          0          0
14          0          1
15          0          1
16          0          1
17          0          1
18          0          1
19          0          1
20          0          0
21          0          1
22          0          1
23          0          1
24          0          1
25          0          0
26          0          1
27          0          1
28          0          1
29          0          1
..        ...        ...
470         1          1
471         1          1
472         1          1
473         1          1
474         1          1
475         1          1
476         1          1
477         1          1
478         1          1
479         1          1
480         1          1
481         1          1
482         1          1
483         1          1
484         1          0
485         1          1
486         1          1
487         1          1
488         1          1
489         1          1
490         1          1
491         1          1
492         1          1
493         1          1
494         1          1
495         1          1
496         1          1
497         1          1
498         1          1
499         1          1

[500 rows x 2 columns]

*** Feature weigths ***
 {'body_bow': 0.2, 'lexicon_features': 0.2, 'pos_features': 0.0, 'liwc_features': 0.0, 'surface_features': 0.2, 'readability_features': 0.0}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'fit_time': array([ 54.80964994,  53.19358706,  50.54957104]), 'test_score': array([ 0.47685185,  0.55092593,  0.46261682]), 'train_score': array([ 0.55581395,  0.51627907,  0.57175926]), 'score_time': array([ 24.16491914,  25.73453712,  28.66442895])}
 Mean train score:  0.547950760838
 Mean test score:  0.496798200069
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.57      0.44      0.50       323
           1       0.54      0.66      0.59       323

   micro avg       0.55      0.55      0.55       646
   macro avg       0.55      0.55      0.55       646
weighted avg       0.55      0.55      0.55       646

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.58      0.38      0.46        48
           1       0.51      0.70      0.59        44

   micro avg       0.53      0.53      0.53        92
   macro avg       0.54      0.54      0.52        92
weighted avg       0.55      0.53      0.52        92

confusion matrix:
[[18 30]
 [13 31]]
    Expected  Predicted
0          0          0
1          0          0
2          0          1
3          0          1
4          0          0
5          0          1
6          0          1
7          0          1
8          0          0
9          0          1
10         0          1
11         0          0
12         0          1
13         0          1
14         0          0
15         0          1
16         0          1
17         0          1
18         0          0
19         0          0
20         0          0
21         0          0
22         0          0
23         0          1
24         0          0
25         0          0
26         0          1
27         0          0
28         0          0
29         0          1
..       ...        ...
62         1          0
63         1          1
64         1          1
65         1          1
66         1          1
67         1          0
68         1          1
69         1          1
70         1          1
71         1          1
72         1          1
73         1          1
74         1          1
75         1          1
76         1          0
77         1          1
78         1          1
79         1          1
80         1          1
81         1          1
82         1          1
83         1          1
84         1          1
85         1          1
86         1          0
87         1          0
88         1          0
89         1          1
90         1          1
91         1          0

[92 rows x 2 columns]
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.88      0.94        33

   micro avg       0.88      0.88      0.88        33
   macro avg       0.50      0.44      0.47        33
weighted avg       1.00      0.88      0.94        33

confusion matrix:
[[ 0  0]
 [ 4 29]]
    Expected  Predicted
0          1          1
1          1          1
2          1          1
3          1          0
4          1          1
5          1          1
6          1          1
7          1          1
8          1          1
9          1          1
10         1          1
11         1          1
12         1          0
13         1          1
14         1          1
15         1          1
16         1          1
17         1          1
18         1          1
19         1          0
20         1          1
21         1          1
22         1          1
23         1          1
24         1          1
25         1          0
26         1          1
27         1          1
28         1          1
29         1          1
30         1          1
31         1          1
32         1          1
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
              precision    recall  f1-score   support

           0       0.72      0.21      0.32       250
           1       0.54      0.92      0.68       250

   micro avg       0.56      0.56      0.56       500
   macro avg       0.63      0.56      0.50       500
weighted avg       0.63      0.56      0.50       500

confusion matrix:
[[ 52 198]
 [ 20 230]]
     Expected  Predicted
0           0          1
1           0          1
2           0          1
3           0          1
4           0          0
5           0          1
6           0          1
7           0          1
8           0          0
9           0          1
10          0          1
11          0          1
12          0          1
13          0          0
14          0          1
15          0          1
16          0          1
17          0          1
18          0          1
19          0          1
20          0          0
21          0          1
22          0          1
23          0          1
24          0          1
25          0          0
26          0          1
27          0          1
28          0          1
29          0          1
..        ...        ...
470         1          1
471         1          1
472         1          1
473         1          1
474         1          1
475         1          1
476         1          1
477         1          1
478         1          1
479         1          1
480         1          1
481         1          1
482         1          1
483         1          1
484         1          0
485         1          1
486         1          1
487         1          1
488         1          1
489         1          1
490         1          1
491         1          1
492         1          1
493         1          1
494         1          1
495         1          1
496         1          1
497         1          1
498         1          1
499         1          1

[500 rows x 2 columns]

*** Feature weigths ***
 {'body_bow': 0.2, 'lexicon_features': 0.0, 'pos_features': 0.2, 'liwc_features': 0.0, 'surface_features': 0.2, 'readability_features': 0.0}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'fit_time': array([ 54.79321218,  53.1320641 ,  50.60535884]), 'test_score': array([ 0.47685185,  0.55092593,  0.44392523]), 'train_score': array([ 0.55581395,  0.51627907,  0.57638889]), 'score_time': array([ 24.22820187,  25.78858781,  28.63224697])}
 Mean train score:  0.549493970715
 Mean test score:  0.490567670474
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.57      0.44      0.50       323
           1       0.54      0.66      0.59       323

   micro avg       0.55      0.55      0.55       646
   macro avg       0.55      0.55      0.55       646
weighted avg       0.55      0.55      0.55       646

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.58      0.38      0.46        48
           1       0.51      0.70      0.59        44

   micro avg       0.53      0.53      0.53        92
   macro avg       0.54      0.54      0.52        92
weighted avg       0.55      0.53      0.52        92

confusion matrix:
[[18 30]
 [13 31]]
    Expected  Predicted
0          0          0
1          0          0
2          0          1
3          0          1
4          0          0
5          0          1
6          0          1
7          0          1
8          0          0
9          0          1
10         0          1
11         0          0
12         0          1
13         0          1
14         0          0
15         0          1
16         0          1
17         0          1
18         0          0
19         0          0
20         0          0
21         0          0
22         0          0
23         0          1
24         0          0
25         0          0
26         0          1
27         0          0
28         0          0
29         0          1
..       ...        ...
62         1          0
63         1          1
64         1          1
65         1          1
66         1          1
67         1          0
68         1          1
69         1          1
70         1          1
71         1          1
72         1          1
73         1          1
74         1          1
75         1          1
76         1          0
77         1          1
78         1          1
79         1          1
80         1          1
81         1          1
82         1          1
83         1          1
84         1          1
85         1          1
86         1          0
87         1          0
88         1          0
89         1          1
90         1          1
91         1          0

[92 rows x 2 columns]
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.88      0.94        33

   micro avg       0.88      0.88      0.88        33
   macro avg       0.50      0.44      0.47        33
weighted avg       1.00      0.88      0.94        33

confusion matrix:
[[ 0  0]
 [ 4 29]]
    Expected  Predicted
0          1          1
1          1          1
2          1          1
3          1          0
4          1          1
5          1          1
6          1          1
7          1          1
8          1          1
9          1          1
10         1          1
11         1          1
12         1          0
13         1          1
14         1          1
15         1          1
16         1          1
17         1          1
18         1          1
19         1          0
20         1          1
21         1          1
22         1          1
23         1          1
24         1          1
25         1          0
26         1          1
27         1          1
28         1          1
29         1          1
30         1          1
31         1          1
32         1          1
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
              precision    recall  f1-score   support

           0       0.72      0.21      0.32       250
           1       0.54      0.92      0.68       250

   micro avg       0.56      0.56      0.56       500
   macro avg       0.63      0.56      0.50       500
weighted avg       0.63      0.56      0.50       500

confusion matrix:
[[ 52 198]
 [ 20 230]]
     Expected  Predicted
0           0          1
1           0          1
2           0          1
3           0          1
4           0          0
5           0          1
6           0          1
7           0          1
8           0          0
9           0          1
10          0          1
11          0          1
12          0          1
13          0          0
14          0          1
15          0          1
16          0          1
17          0          1
18          0          1
19          0          1
20          0          0
21          0          1
22          0          1
23          0          1
24          0          1
25          0          0
26          0          1
27          0          1
28          0          1
29          0          1
..        ...        ...
470         1          1
471         1          1
472         1          1
473         1          1
474         1          1
475         1          1
476         1          1
477         1          1
478         1          1
479         1          1
480         1          1
481         1          1
482         1          1
483         1          1
484         1          0
485         1          1
486         1          1
487         1          1
488         1          1
489         1          1
490         1          1
491         1          1
492         1          1
493         1          1
494         1          1
495         1          1
496         1          1
497         1          1
498         1          1
499         1          1

[500 rows x 2 columns]

*** Feature weigths ***
 {'body_bow': 0.2, 'lexicon_features': 0.2, 'pos_features': 0.2, 'liwc_features': 0.0, 'surface_features': 0.0, 'readability_features': 0.0}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'fit_time': array([ 54.83095908,  53.20501208,  50.508358  ]), 'test_score': array([ 0.58333333,  0.59722222,  0.60747664]), 'train_score': array([ 0.96744186,  0.96511628,  0.96064815]), 'score_time': array([ 24.36210012,  25.73588395,  28.63030601])}
 Mean train score:  0.964402095894
 Mean test score:  0.596010730357
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.93      0.97      0.95       323
           1       0.97      0.93      0.95       323

   micro avg       0.95      0.95      0.95       646
   macro avg       0.95      0.95      0.95       646
weighted avg       0.95      0.95      0.95       646

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.69      0.75      0.72        48
           1       0.70      0.64      0.67        44

   micro avg       0.70      0.70      0.70        92
   macro avg       0.70      0.69      0.69        92
weighted avg       0.70      0.70      0.69        92

confusion matrix:
[[36 12]
 [16 28]]
    Expected  Predicted
0          0          0
1          0          0
2          0          1
3          0          0
4          0          0
5          0          0
6          0          0
7          0          0
8          0          0
9          0          1
10         0          0
11         0          0
12         0          0
13         0          0
14         0          0
15         0          0
16         0          0
17         0          0
18         0          0
19         0          0
20         0          1
21         0          1
22         0          0
23         0          0
24         0          0
25         0          0
26         0          0
27         0          1
28         0          1
29         0          0
..       ...        ...
62         1          1
63         1          1
64         1          1
65         1          1
66         1          1
67         1          1
68         1          0
69         1          1
70         1          1
71         1          0
72         1          0
73         1          1
74         1          1
75         1          1
76         1          0
77         1          1
78         1          0
79         1          1
80         1          1
81         1          1
82         1          1
83         1          0
84         1          1
85         1          1
86         1          0
87         1          1
88         1          1
89         1          1
90         1          1
91         1          0

[92 rows x 2 columns]
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.88      0.94        33

   micro avg       0.88      0.88      0.88        33
   macro avg       0.50      0.44      0.47        33
weighted avg       1.00      0.88      0.94        33

confusion matrix:
[[ 0  0]
 [ 4 29]]
    Expected  Predicted
0          1          1
1          1          1
2          1          1
3          1          1
4          1          1
5          1          1
6          1          1
7          1          1
8          1          1
9          1          1
10         1          1
11         1          1
12         1          1
13         1          1
14         1          1
15         1          1
16         1          1
17         1          1
18         1          1
19         1          0
20         1          0
21         1          1
22         1          1
23         1          1
24         1          1
25         1          1
26         1          1
27         1          1
28         1          0
29         1          1
30         1          1
31         1          1
32         1          0
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
              precision    recall  f1-score   support

           0       0.68      0.54      0.61       250
           1       0.62      0.75      0.68       250

   micro avg       0.65      0.65      0.65       500
   macro avg       0.65      0.65      0.64       500
weighted avg       0.65      0.65      0.64       500

confusion matrix:
[[136 114]
 [ 63 187]]
     Expected  Predicted
0           0          1
1           0          1
2           0          1
3           0          1
4           0          1
5           0          1
6           0          1
7           0          1
8           0          1
9           0          1
10          0          0
11          0          0
12          0          1
13          0          1
14          0          1
15          0          1
16          0          1
17          0          1
18          0          1
19          0          0
20          0          0
21          0          1
22          0          1
23          0          0
24          0          1
25          0          1
26          0          0
27          0          0
28          0          1
29          0          1
..        ...        ...
470         1          1
471         1          1
472         1          1
473         1          1
474         1          0
475         1          0
476         1          1
477         1          0
478         1          0
479         1          1
480         1          1
481         1          1
482         1          1
483         1          0
484         1          0
485         1          1
486         1          1
487         1          1
488         1          1
489         1          1
490         1          1
491         1          1
492         1          0
493         1          1
494         1          0
495         1          1
496         1          1
497         1          1
498         1          1
499         1          1

[500 rows x 2 columns]

*** Feature weigths ***
 {'body_bow': 0.2, 'lexicon_features': 0.2, 'pos_features': 0.2, 'liwc_features': 0.2, 'surface_features': 0.2, 'readability_features': 0.0}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'fit_time': array([ 54.83072495,  53.30201793,  50.46460414]), 'test_score': array([ 0.48611111,  0.55092593,  0.44392523]), 'train_score': array([ 0.55581395,  0.51860465,  0.54398148]), 'score_time': array([ 24.31242704,  25.79102111,  28.64481783])}
 Mean train score:  0.539466695378
 Mean test score:  0.493654090227
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.57      0.44      0.50       323
           1       0.54      0.66      0.59       323

   micro avg       0.55      0.55      0.55       646
   macro avg       0.55      0.55      0.55       646
weighted avg       0.55      0.55      0.55       646

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.58      0.38      0.46        48
           1       0.51      0.70      0.59        44

   micro avg       0.53      0.53      0.53        92
   macro avg       0.54      0.54      0.52        92
weighted avg       0.55      0.53      0.52        92

confusion matrix:
[[18 30]
 [13 31]]
    Expected  Predicted
0          0          0
1          0          0
2          0          1
3          0          1
4          0          0
5          0          1
6          0          1
7          0          1
8          0          0
9          0          1
10         0          1
11         0          0
12         0          1
13         0          1
14         0          0
15         0          1
16         0          1
17         0          1
18         0          0
19         0          0
20         0          0
21         0          0
22         0          0
23         0          1
24         0          0
25         0          0
26         0          1
27         0          0
28         0          0
29         0          1
..       ...        ...
62         1          0
63         1          1
64         1          1
65         1          1
66         1          1
67         1          0
68         1          1
69         1          1
70         1          1
71         1          1
72         1          1
73         1          1
74         1          1
75         1          1
76         1          0
77         1          1
78         1          1
79         1          1
80         1          1
81         1          1
82         1          1
83         1          1
84         1          1
85         1          1
86         1          0
87         1          0
88         1          0
89         1          1
90         1          1
91         1          0

[92 rows x 2 columns]
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.88      0.94        33

   micro avg       0.88      0.88      0.88        33
   macro avg       0.50      0.44      0.47        33
weighted avg       1.00      0.88      0.94        33

confusion matrix:
[[ 0  0]
 [ 4 29]]
    Expected  Predicted
0          1          1
1          1          1
2          1          1
3          1          0
4          1          1
5          1          1
6          1          1
7          1          1
8          1          1
9          1          1
10         1          1
11         1          1
12         1          0
13         1          1
14         1          1
15         1          1
16         1          1
17         1          1
18         1          1
19         1          0
20         1          1
21         1          1
22         1          1
23         1          1
24         1          1
25         1          0
26         1          1
27         1          1
28         1          1
29         1          1
30         1          1
31         1          1
32         1          1
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
              precision    recall  f1-score   support

           0       0.72      0.21      0.32       250
           1       0.54      0.92      0.68       250

   micro avg       0.56      0.56      0.56       500
   macro avg       0.63      0.56      0.50       500
weighted avg       0.63      0.56      0.50       500

confusion matrix:
[[ 52 198]
 [ 20 230]]
     Expected  Predicted
0           0          1
1           0          1
2           0          1
3           0          1
4           0          0
5           0          1
6           0          1
7           0          1
8           0          0
9           0          1
10          0          1
11          0          1
12          0          1
13          0          0
14          0          1
15          0          1
16          0          1
17          0          1
18          0          1
19          0          1
20          0          0
21          0          1
22          0          1
23          0          1
24          0          1
25          0          0
26          0          1
27          0          1
28          0          1
29          0          1
..        ...        ...
470         1          1
471         1          1
472         1          1
473         1          1
474         1          1
475         1          1
476         1          1
477         1          1
478         1          1
479         1          1
480         1          1
481         1          1
482         1          1
483         1          1
484         1          0
485         1          1
486         1          1
487         1          1
488         1          1
489         1          1
490         1          1
491         1          1
492         1          1
493         1          1
494         1          1
495         1          1
496         1          1
497         1          1
498         1          1
499         1          1

[500 rows x 2 columns]

*** Feature weigths ***
 {'body_bow': 0.2, 'lexicon_features': 0.2, 'pos_features': 0.0, 'liwc_features': 0.2, 'surface_features': 0.2, 'readability_features': 0.0}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'fit_time': array([ 54.8262949 ,  53.19939208,  50.51844192]), 'test_score': array([ 0.48611111,  0.55092593,  0.48130841]), 'train_score': array([ 0.55813953,  0.51627907,  0.65740741]), 'score_time': array([ 24.17375493,  25.99158001,  28.58929205])}
 Mean train score:  0.577275337353
 Mean test score:  0.506115149417
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.57      0.44      0.50       323
           1       0.54      0.66      0.59       323

   micro avg       0.55      0.55      0.55       646
   macro avg       0.55      0.55      0.55       646
weighted avg       0.55      0.55      0.55       646

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.58      0.38      0.46        48
           1       0.51      0.70      0.59        44

   micro avg       0.53      0.53      0.53        92
   macro avg       0.54      0.54      0.52        92
weighted avg       0.55      0.53      0.52        92

confusion matrix:
[[18 30]
 [13 31]]
    Expected  Predicted
0          0          0
1          0          0
2          0          1
3          0          1
4          0          0
5          0          1
6          0          1
7          0          1
8          0          0
9          0          1
10         0          1
11         0          0
12         0          1
13         0          1
14         0          0
15         0          1
16         0          1
17         0          1
18         0          0
19         0          0
20         0          0
21         0          0
22         0          0
23         0          1
24         0          0
25         0          0
26         0          1
27         0          0
28         0          0
29         0          1
..       ...        ...
62         1          0
63         1          1
64         1          1
65         1          1
66         1          1
67         1          0
68         1          1
69         1          1
70         1          1
71         1          1
72         1          1
73         1          1
74         1          1
75         1          1
76         1          0
77         1          1
78         1          1
79         1          1
80         1          1
81         1          1
82         1          1
83         1          1
84         1          1
85         1          1
86         1          0
87         1          0
88         1          0
89         1          1
90         1          1
91         1          0

[92 rows x 2 columns]
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.88      0.94        33

   micro avg       0.88      0.88      0.88        33
   macro avg       0.50      0.44      0.47        33
weighted avg       1.00      0.88      0.94        33

confusion matrix:
[[ 0  0]
 [ 4 29]]
    Expected  Predicted
0          1          1
1          1          1
2          1          1
3          1          0
4          1          1
5          1          1
6          1          1
7          1          1
8          1          1
9          1          1
10         1          1
11         1          1
12         1          0
13         1          1
14         1          1
15         1          1
16         1          1
17         1          1
18         1          1
19         1          0
20         1          1
21         1          1
22         1          1
23         1          1
24         1          1
25         1          0
26         1          1
27         1          1
28         1          1
29         1          1
30         1          1
31         1          1
32         1          1
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
              precision    recall  f1-score   support

           0       0.72      0.21      0.32       250
           1       0.54      0.92      0.68       250

   micro avg       0.56      0.56      0.56       500
   macro avg       0.63      0.56      0.50       500
weighted avg       0.63      0.56      0.50       500

confusion matrix:
[[ 52 198]
 [ 20 230]]
     Expected  Predicted
0           0          1
1           0          1
2           0          1
3           0          1
4           0          0
5           0          1
6           0          1
7           0          1
8           0          0
9           0          1
10          0          1
11          0          1
12          0          1
13          0          0
14          0          1
15          0          1
16          0          1
17          0          1
18          0          1
19          0          1
20          0          0
21          0          1
22          0          1
23          0          1
24          0          1
25          0          0
26          0          1
27          0          1
28          0          1
29          0          1
..        ...        ...
470         1          1
471         1          1
472         1          1
473         1          1
474         1          1
475         1          1
476         1          1
477         1          1
478         1          1
479         1          1
480         1          1
481         1          1
482         1          1
483         1          1
484         1          0
485         1          1
486         1          1
487         1          1
488         1          1
489         1          1
490         1          1
491         1          1
492         1          1
493         1          1
494         1          1
495         1          1
496         1          1
497         1          1
498         1          1
499         1          1

[500 rows x 2 columns]

Process finished with exit code 0



/Users/fa/anaconda/envs/py35/bin/python /Users/fa/workspace/shared/sfu/fake_news/src/union_classification_explore.py
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.
  _nan_object_mask = _nan_object_array != _nan_object_array
Preparing lexicons & lwicDic
Number of terms in the lexicon act_adverbs.txt : 15
Number of terms in the lexicon assertives_hooper1975.txt : 67
Number of terms in the lexicon comparative_forms.txt : 2122
Number of terms in the lexicon factives_hooper1975.txt : 29
Number of terms in the lexicon hedges_hyland2005.txt : 105
Number of terms in the lexicon implicatives_karttunen1971.txt : 32
Number of terms in the lexicon manner_adverbs.txt : 128
Number of terms in the lexicon modal_adverbs.txt : 94
Number of terms in the lexicon negative-HuLui.txt : 4784
Number of terms in the lexicon negative_mpqa.txt : 3078
Number of terms in the lexicon neutral_mpqa.txt : 175
Number of terms in the lexicon posative_mpqa.txt : 2304
Number of terms in the lexicon positive-HuLui.txt : 2007
Number of terms in the lexicon report_verbs.txt : 181
Number of terms in the lexicon superlative_forms.txt : 2306
Data loaded from disk!
Size of train, validataion and test sets: 646 , 0 , 0
0    Clinton gets it wrong on 'small business' job ...
1     Uber launched a fleet of its much anticipated...
2    Story highlights US State Department commends ...
dtype: object
0    0
1    0
2    0
dtype: int64
Loading data snopes...
(118, 5)
0    mixture
1     mfalse
2    mixture
3    mixture
4      mtrue
5     ffalse
6     mfalse
7      mtrue
8     ffalse
9     ffalse
Name: label, dtype: object
['mixture' 'mfalse' 'mtrue' 'ffalse' 'ftrue']
Data from Snopes looks like...
0     Massive Pedophile Ring With '70,000 Elite Mem...
1     NBC Bay Area's SkyRanger on Saturday captured...
2     Today is Flag Day, the anniversary of when 19...
3     HONOLULU Federal authorities on Friday added ...
4     Text smaller    Text bigger    We have all he...
5     A 79-year-old retired officer of the CIA, Bil...
6     Governor Jerry Brown is retiring but not befo...
7     Sneed: 108 could be the Cubs magic number thi...
8     Clint Eastwood, more famous for westerns than...
9     Delaware City Council passed a resolution thi...
Name: data, dtype: object
[5, 1, 5, 5, 0, 1, 1, 0, 1, 1]
0    48
1    44
5    26
dtype: int64
Loading data buzzfeedtop...
(33, 15)
                                               title  \
0  Babysitter transported to hospital after inser...
1  FBI seizes over 3,000 penises during raid at m...
2  Charles Manson to be released on parole, to Jo...

                                                 url Politifact  \
0  http://worldnewsdailyreport.com/babysitter-tra...        NaN
1  http://worldnewsdailyreport.com/fbi-seizes-ove...        NaN
2  http://www.breakingnews365.net/59690fb994b9c/c...        NaN

   Politifact FB                                             Snopes  \
0            NaN  https://www.snopes.com/babysitter-transported-...
1            NaN  https://www.snopes.com/fbi-seizes-3000-penises...
2            NaN  https://www.snopes.com/politics/satire/mansonp...

   Snopes FB Factcheck  Factcheck FB  ABC  ABC FB error_phase2  \
0     1734.0       NaN           NaN  NaN     NaN     No Error
1       14.0       NaN           NaN  NaN     NaN     No Error
2        NaN       NaN           NaN  NaN     NaN     No Error

                        original_article_text_phase2  \
0  Cincinnati, Ohio | A 31-year old woman was adm...
1  FBI agents made an astonishing discovery this ...
2  Giant Squid Washes Ashore on Lake Michigan Mic...

                                article_title_phase2 publish_date_phase2  \
0  Babysitter transported to hospital after inser...          2017-05-03
1  FBI seizes over 3,000 penises during raid at m...          2017-09-25
2  Charles Manson to be released on parole, to Jo...                 NaN

       author_phase2
0  Barbara Jennnings
1  Barbara Jennnings
2                NaN
Data from BuzzFeed looks like...
0    Cincinnati, Ohio | A 31-year old woman was adm...
1    FBI agents made an astonishing discovery this ...
2    Giant Squid Washes Ashore on Lake Michigan Mic...
3    Police have reportedly launched a murder inves...
4    Beaumont, Texas | An employee of the Jefferson...
5    WASHINGTON, DC (By J. McConkey)A group of lead...
6    WASHINGTON, D.C.  In another sweeping move aim...
7    Columbus, Ohio | An 83-year old woman was arre...
8    A couple was transported to the hospital in a ...
9    Darrel Whitaker from Glenwood Springs in Color...
Name: original_article_text_phase2, dtype: object
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
1    33
dtype: int64
Loading data perez...
(500, 3)
   Unnamed: 0                                               text  label
0           0  Jennifer Aniston dashes 'Friends' reunion hope...  legit
1           1  This Is What Brad Pitt Has Been Texting Jennif...  legit
2           2  Jennifer Aniston's spokesman denies reports th...  legit
Data from perez looks like...
0    Jennifer Aniston dashes 'Friends' reunion hope...
1    This Is What Brad Pitt Has Been Texting Jennif...
2    Jennifer Aniston's spokesman denies reports th...
3    Jennifer Aniston sparks adoption rumors\n\nBef...
4    Jennifer Aniston denies she had an affair with...
5    Jennifer Aniston: I'm Not a 'Sad, Childless Hu...
6    Jennifer Aniston Finally Pregnant At 48 Years ...
7    Brad Pitt is not reuniting with Jennifer Anist...
8    Miley And Liam Fighting? False Rumors Swirl Th...
9    Kristen Stewart not dropping another "Twilight...
Name: text, dtype: object
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
1    250
0    250
dtype: int64

*** Feature weigths ***
 {'surface_features': 0.0, 'pos_features': 0.2, 'body_bow': 0.2, 'liwc_features': 0.2, 'readability_features': 0.0, 'lexicon_features': 0.2}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'train_score': array([ 0.96046512,  0.9627907 ,  0.94675926]), 'score_time': array([ 26.47273707,  27.53659916,  30.51792097]), 'fit_time': array([ 61.51416993,  57.13952279,  54.20142198]), 'test_score': array([ 0.5787037 ,  0.61574074,  0.59813084])}
 Mean train score:  0.956671691071
 Mean test score:  0.597525095189
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.92      0.97      0.94       323
           1       0.96      0.92      0.94       323

   micro avg       0.94      0.94      0.94       646
   macro avg       0.94      0.94      0.94       646
weighted avg       0.94      0.94      0.94       646

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.69      0.77      0.73        48
           1       0.71      0.61      0.66        44

   micro avg       0.70      0.70      0.70        92
   macro avg       0.70      0.69      0.69        92
weighted avg       0.70      0.70      0.69        92

confusion matrix:
[[37 11]
 [17 27]]
    Expected  Predicted
0          0          0
1          0          0
2          0          1
3          0          0
4          0          0
5          0          0
6          0          0
7          0          0
8          0          0
9          0          1
10         0          0
11         0          0
12         0          0
13         0          0
14         0          0
15         0          0
16         0          0
17         0          0
18         0          0
19         0          0
20         0          1
21         0          1
22         0          0
23         0          0
24         0          0
25         0          0
26         0          0
27         0          1
28         0          1
29         0          0
..       ...        ...
62         1          1
63         1          1
64         1          1
65         1          1
66         1          1
67         1          1
68         1          0
69         1          1
70         1          1
71         1          0
72         1          0
73         1          1
74         1          1
75         1          1
76         1          0
77         1          1
78         1          0
79         1          1
80         1          1
81         1          1
82         1          1
83         1          0
84         1          1
85         1          1
86         1          0
87         1          1
88         1          1
89         1          1
90         1          1
91         1          0

[92 rows x 2 columns]
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.88      0.94        33

   micro avg       0.88      0.88      0.88        33
   macro avg       0.50      0.44      0.47        33
weighted avg       1.00      0.88      0.94        33

confusion matrix:
[[ 0  0]
 [ 4 29]]
    Expected  Predicted
0          1          1
1          1          1
2          1          1
3          1          1
4          1          1
5          1          1
6          1          1
7          1          1
8          1          1
9          1          1
10         1          1
11         1          1
12         1          1
13         1          1
14         1          1
15         1          1
16         1          1
17         1          1
18         1          1
19         1          0
20         1          0
21         1          1
22         1          1
23         1          1
24         1          1
25         1          1
26         1          1
27         1          1
28         1          0
29         1          1
30         1          1
31         1          1
32         1          0
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.70      0.54      0.61       250
           1       0.62      0.77      0.69       250

   micro avg       0.65      0.65      0.65       500
   macro avg       0.66      0.65      0.65       500
weighted avg       0.66      0.65      0.65       500

confusion matrix:
[[134 116]
 [ 58 192]]
     Expected  Predicted
0           0          1
1           0          1
2           0          1
3           0          1
4           0          1
5           0          1
6           0          1
7           0          1
8           0          1
9           0          1
10          0          0
11          0          0
12          0          1
13          0          0
14          0          1
15          0          1
16          0          1
17          0          1
18          0          1
19          0          0
20          0          0
21          0          1
22          0          1
23          0          0
24          0          1
25          0          1
26          0          0
27          0          0
28          0          1
29          0          1
..        ...        ...
470         1          1
471         1          1
472         1          1
473         1          1
474         1          0
475         1          0
476         1          1
477         1          0
478         1          0
479         1          1
480         1          1
481         1          1
482         1          1
483         1          0
484         1          0
485         1          1
486         1          1
487         1          1
488         1          1
489         1          1
490         1          1
491         1          1
492         1          0
493         1          1
494         1          0
495         1          1
496         1          1
497         1          1
498         1          1
499         1          1

[500 rows x 2 columns]

Process finished with exit code 0
