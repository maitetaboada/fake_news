/Users/fa/anaconda/envs/py35/bin/python /Users/fa/workspace/shared/sfu/fake_news/src/union_classification_labelMapping.py
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.
  _nan_object_mask = _nan_object_array != _nan_object_array
Preparing lexicons & lwicDic
Number of terms in the lexicon act_adverbs.txt : 15
Number of terms in the lexicon assertives_hooper1975.txt : 67
Number of terms in the lexicon comparative_forms.txt : 2122
Number of terms in the lexicon factives_hooper1975.txt : 29
Number of terms in the lexicon hedges_hyland2005.txt : 105
Number of terms in the lexicon implicatives_karttunen1971.txt : 32
Number of terms in the lexicon manner_adverbs.txt : 128
Number of terms in the lexicon modal_adverbs.txt : 94
Number of terms in the lexicon negative-HuLui.txt : 4784
Number of terms in the lexicon negative_mpqa.txt : 3078
Number of terms in the lexicon neutral_mpqa.txt : 175
Number of terms in the lexicon posative_mpqa.txt : 2304
Number of terms in the lexicon positive-HuLui.txt : 2007
Number of terms in the lexicon report_verbs.txt : 181
Number of terms in the lexicon superlative_forms.txt : 2306
Data loaded from disk!
Size of train and test sets: 8000 , 0
[ "The proposed trade agreement with China will be helpful for Taiwan despite its complicated nature making negotiations difficult and time-consuming, Taiwan's top cross-strait negotiator said Tuesday. 'This is going to be a bargain no matter how you calculate it, ' said Chiang Pin-kung, chairman of the Straits Exchange Foundation (SEF) -- Taiwan's quasi- official organization that deals with cross-strait matters -- while addressing reporters on the proposed economic cooperation framework agreement (ECFA). Negotiation of the ECFA, which is aimed at improving bilateral economic ties and lowering trade barriers, is expected to enter its second round later this month. 'Taiwan's economy is driven by exports. The key for Taiwanese businesses is receiving fair treatment in foreign markets. If they do, they will be fine, ' Chiang said, adding that 'the saddest thing is when you're competitive but not treated fairly.' The talks have been progressing as expected, but have entered the 'difficult' part of President Ma Ying-jeou's 'first the easy, then the difficult' cross- strait policy, he said. Given the complexity of the trade pact, which involves many government agencies, negotiations are expected to take some time, he said. The veteran politician said that Taiwan and China had common interests when they negotiated for the 'mini three links, ' but the situation was different with the ECFA because both sides see advantages and disadvantages at the same time. The 'mini three links' negotiations led to limited connections between Taiwan's Kinmen and Matsu and several cities in China's Fujian province. Asked about the lack of consensus from the public and opposition from certain domestic sectors, Chiang said that Taiwan was forced to open its market because of World Trade Organization (WTO) stipulations. Negotiations for WTO accession lasted almost 10 years and involved more than 30 countries, thousands of tariff elimination items and more than 240 meetings, he said. 'We lowered the average tariffs on our industrial products from 7 percent to 4 percent as well as those of some agricultural products from 20 percent to 12 percent. Did Taiwan businesses collapse? No, ' he said. In fact, he said, Taiwan's exports increased significantly after WTO accession. He said that Taiwanese businesses suffered badly from 1986 to 1988, when the New Taiwan dollar appreciated by 40 percent relative to the U.S. dollar -- from NT$40 to US$1 to NT$25 to US$1. 'But Taiwan businesses survived, ' he said. Japan has taken notice of what Taiwan is doing, Chiang said, and now places the signing of free trade agreements (FTAs) at the top of its national economy policy. Japan has always wanted to formulate its economic policy through the mechanism of the WTO, he said, but is now resorting to bilateral FTAs or regional FTAs after the stalled Doha talks and Taiwan's aggressiveness in seeking a trade agreement with China. Chiang admitted that domestic opposition to the deal is still fierce, but said there is no way to reach consensus except through better communication with the people, which the government has been working on relentlessly. 'At the end of the day... without it (the ECFA) , Taiwan's trade volume will be taken away and domestic and foreign investments will move elsewhere,' he said. "
 "The program, launched after the terrorist attacks of Sept. 11, 2001, was ended by new agency Director Leon E. Panetta last month shortly after he learned about it and before it became operational. The investigation is aimed at determining whether CIA or other officials violated laws that require the executive branch to keep Congress fully informed of 'significant' intelligence activities. It opens a new front in the ongoing scrutiny of the agency's counterterrorism efforts under the Bush administration. Members of Congress didn't learn about the program until June 24, when Panetta arranged emergency briefings with the intelligence committees in both chambers. At the time, Panetta told lawmakers that Cheney had instructed the CIA not to share information about the program with Congress. Aides to Cheney have not responded to requests for comment. But former Bush administration officials have disputed characterizations of Cheney's involvement, saying that his comments came early in the program and that he merely encouraged the CIA not to discuss the effort until it was clear that it would go forward. CIA officials struggled for years to overcome legal and logistical obstacles, including who should be selected for the missions, where they would be trained and based, and how they might be extracted if they were successful. After shelving the program several times, agency officials revived the idea in recent years in hopes that the teams could also gather intelligence on the ground in Pakistan that could provide new clues to the whereabouts of Osama bin Laden and other al-Qaida leaders. The House probe comes at a time of intense political fighting on Capitol Hill over Bush-era counterterrorism measures. Earlier this year, House Speaker Nancy Pelosi, D-Calif., accused the CIA of misleading Congress about the brutal interrogation methods it had employed after the Sept. 11 attacks. Rep. Pete Hoekstra, R-Mich., the top Republican on the House committee, called the investigation a 'partisan plan' by Democrats to provide political cover for Pelosi. 'They are putting their partisan conclusions ahead of the facts,' Hoekstra said, noting that Director of National Intelligence Dennis Blair indicated earlier this week that he agreed there was no need to brief Congress on the program because it never got off the ground. CIA spokesman Paul Gimigliano noted that Committee Chairman Silvestre Reyes, D-Texas, had pledged to avoid allowing the investigation to become a 'distraction' to CIA employees, and said the agency would 'work closely with the committee on this review.' "
 "A Taiwanese delegation will depart for Seoul Friday to attend a meeting of the World Taekwondo Federation (WTF) disciplinary committee on an Asian Games disqualification dispute involving one of Taiwan's top competitors. Among the delegation members is Yang Shu-chun, who was disqualified from the women's under 49-kilogram division in the taekwondo competition at the Asiad in Guangzhou, China on Nov. 17 for allegedly wearing extra sensors in her socks in an attempt to score more points. Yang will be accompanied by Chinese Taipei Taekwondo Association (CTTA) President Chen Chien-ping and her two coaches, Liu Ching-wen and Liu Tsung-ta, CTTA Secretary-General Ho Feng-yen said Monday. 'They will attend a WTF disciplinary committee meeting scheduled for Dec. 18 and present written testimony in hopes of getting a fair solution to the dispute, ' Ho said, adding that the delegation will return to Taipei that same day. The 25-year-old athlete, who was one Taiwan's top hopes for a gold medal at the Asiad, was disqualified during her opening bout for using the extra sensors, according to taekwondo officials. Video footage of the match released later showed, however, that Yang was not wearing the sensors during the bout. Inconsistent explanations by WTF and Asian Taekwondo Union (ATU) officials of why Yang was disqualified have added to the controversy. Taiwan filed a protest over the decision with the Olympic Council of Asia (OCA) but also vowed to take the case to the Court of Arbitration for Sport if other appeals failed to provide a satisfactory outcome. The Sports Affairs Council (SAC) commissioned the law firm Lee and Li Attorneys- at-Law to file an appeal with the Court of Arbitration for Sport (CAS) on Dec. 8 for Yang's controversial dismissal. SAC Vice Minister Steven S. K. Chen said last Friday that Taiwan could withdraw the arbitration appeal if the WTF issues an acceptable statement after its Dec. 18 meeting. Major South Korean newspapers have prominently covered the disqualification row, which they said has stalked anti-Korean sentiment among ordinary Taiwanese people. The sentiment was exacerbated by reports over the weekend that Korean electronics company Samsung provided evidence in the European Union's price- fixing probe against four Taiwanese display panel makers that led to stiff fines against them. Samsung, the world's largest flat panel maker, was also allegedly part of the price-fixing cartel and may have even led the group, but because it tipped off authorities in Europe to the behavior, it was not disciplined in the case. The Korean news agency Yonhap said in an editorial Friday that Taiwan's anti- Korean sentiment could pose a diplomatic crisis for South Korea. It urged the Korean government to consider how to maintain friendly ties with Taiwan in the wake of these incidents. "]
[0 0 0]
Loading data snopes...
(118, 5)
0    mixture
1     mfalse
2    mixture
3    mixture
4      mtrue
5     ffalse
6     mfalse
7      mtrue
8     ffalse
9     ffalse
Name: label, dtype: object
['mixture' 'mfalse' 'mtrue' 'ffalse' 'ftrue']
Data from Snopes looks like...
0     Massive Pedophile Ring With '70,000 Elite Mem...
1     NBC Bay Area's SkyRanger on Saturday captured...
2     Today is Flag Day, the anniversary of when 19...
3     HONOLULU Federal authorities on Friday added ...
4     Text smaller    Text bigger    We have all he...
5     A 79-year-old retired officer of the CIA, Bil...
6     Governor Jerry Brown is retiring but not befo...
7     Sneed: 108 could be the Cubs magic number thi...
8     Clint Eastwood, more famous for westerns than...
9     Delaware City Council passed a resolution thi...
Name: data, dtype: object
[5, 1, 5, 5, 0, 1, 1, 0, 1, 1]
0    48
1    44
5    26
dtype: int64
Loading data buzzfeedtop...
(33, 15)
                                               title  \
0  Babysitter transported to hospital after inser...
1  FBI seizes over 3,000 penises during raid at m...
2  Charles Manson to be released on parole, to Jo...

                                                 url Politifact  \
0  http://worldnewsdailyreport.com/babysitter-tra...        NaN
1  http://worldnewsdailyreport.com/fbi-seizes-ove...        NaN
2  http://www.breakingnews365.net/59690fb994b9c/c...        NaN

   Politifact FB                                             Snopes  \
0            NaN  https://www.snopes.com/babysitter-transported-...
1            NaN  https://www.snopes.com/fbi-seizes-3000-penises...
2            NaN  https://www.snopes.com/politics/satire/mansonp...

   Snopes FB Factcheck  Factcheck FB  ABC  ABC FB error_phase2  \
0     1734.0       NaN           NaN  NaN     NaN     No Error
1       14.0       NaN           NaN  NaN     NaN     No Error
2        NaN       NaN           NaN  NaN     NaN     No Error

                        original_article_text_phase2  \
0  Cincinnati, Ohio | A 31-year old woman was adm...
1  FBI agents made an astonishing discovery this ...
2  Giant Squid Washes Ashore on Lake Michigan Mic...

                                article_title_phase2 publish_date_phase2  \
0  Babysitter transported to hospital after inser...          2017-05-03
1  FBI seizes over 3,000 penises during raid at m...          2017-09-25
2  Charles Manson to be released on parole, to Jo...                 NaN

       author_phase2
0  Barbara Jennnings
1  Barbara Jennnings
2                NaN
Data from BuzzFeed looks like...
0    Cincinnati, Ohio | A 31-year old woman was adm...
1    FBI agents made an astonishing discovery this ...
2    Giant Squid Washes Ashore on Lake Michigan Mic...
3    Police have reportedly launched a murder inves...
4    Beaumont, Texas | An employee of the Jefferson...
5    WASHINGTON, DC (By J. McConkey)A group of lead...
6    WASHINGTON, D.C.  In another sweeping move aim...
7    Columbus, Ohio | An 83-year old woman was arre...
8    A couple was transported to the hospital in a ...
9    Darrel Whitaker from Glenwood Springs in Color...
Name: original_article_text_phase2, dtype: object
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
1    33
dtype: int64
Loading data perez...
(500, 3)
   Unnamed: 0                                               text  label
0           0  Jennifer Aniston dashes 'Friends' reunion hope...  legit
1           1  This Is What Brad Pitt Has Been Texting Jennif...  legit
2           2  Jennifer Aniston's spokesman denies reports th...  legit
Data from perez looks like...
0    Jennifer Aniston dashes 'Friends' reunion hope...
1    This Is What Brad Pitt Has Been Texting Jennif...
2    Jennifer Aniston's spokesman denies reports th...
3    Jennifer Aniston sparks adoption rumors\n\nBef...
4    Jennifer Aniston denies she had an affair with...
5    Jennifer Aniston: I'm Not a 'Sad, Childless Hu...
6    Jennifer Aniston Finally Pregnant At 48 Years ...
7    Brad Pitt is not reuniting with Jennifer Anist...
8    Miley And Liam Fighting? False Rumors Swirl Th...
9    Kristen Stewart not dropping another "Twilight...
Name: text, dtype: object
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
1    250
0    250
dtype: int64
Loading data emergent...
(7112, 13)
                                claimId                     claimSlug  \
0  8faeb4b0-c41b-11e4-88c9-eb158a06b9a5    19-million-watches-in-2015
1  8faeb4b0-c41b-11e4-88c9-eb158a06b9a5    19-million-watches-in-2015
2  d54aaf40-b6a8-11e4-8507-b58af63d1078  20-year-old-McDonalds-burger

                                       claimHeadline claimTruthiness  \
0  Claim: Apple will sell 19 million Apple Watche...         unknown
1  Claim: Apple will sell 19 million Apple Watche...         unknown
2  Claim: Two Australian men kept a McDonald's Qu...         unknown

                              articleId  \
0  116a3920-c41c-11e4-883c-a7fa7a3c5066
1  7c11b0a0-c41c-11e4-883c-a7fa7a3c5066
2  8bb10fd0-b6aa-11e4-8507-b58af63d1078

                                          articleUrl  articleVersion  \
0  http://appleinsider.com/articles/15/03/06/bmo-...               1
1  http://www.smarteranalyst.com/2015/03/05/apple...               1
2  https://au.tv.yahoo.com/sunrise/video/watch/26...               1

                       articleVersionId  \
0  11925a90-c41c-11e4-9d52-ed599189429c
1  a49d2d10-c41c-11e4-883c-a7fa7a3c5066
2  a5c8e6e0-b6aa-11e4-8507-b58af63d1078

                                     articleHeadline articleByline  \
0  BMO forecasts 19M Apple Watch sales in 2015, w...   Neil Hughes
1  Apple Inc. (AAPL) Bullish Stance Reiterated at...  Scott Fields
2                              World's oldest burger       Sunrise

  articleStance articleHeadlineStance  \
0     observing             observing
1     observing              ignoring
2           for                   for

                                         articleBody
0  Momentum for the Apple Watch will likely take ...
1  Apple Inc. (NASDAQ:AAPL) received another set ...
2  Mates Casey Dean and Eduard Nitz wish a happy ...
claimTruthiness  false  true  unknown   All
articleStance
against            691    22      110   823
for                359   742      511  1612
ignoring            53    15       25    93
observing          943   705     1236  2884
All               2046  1484     1882  5412
Data from Emergent looks like...
2     Mates Casey Dean and Eduard Nitz wish a happy ...
4     Two Australian men think they may be in posses...
5     A McDonald's burger bought 20 years ago has an...
25    A guru who ordered 400 of his followers to und...
30    Multi-millionaire religious "guru" Gurmeet Ram...
31    HE is adored by millions of followers worldwid...
33    Guru Gurmeet Ram Rahim Singh has 40 to 50 mill...
36    Perth | A 600-pound woman has given birth to a...
63    BEIRUT: Lebanon's interior minister said that ...
78    DNA tests have confirmed that a daughter and a...
Name: articleBody, dtype: object
[5, 5, 5, 5, 5, 5, 5, 1, 5, 5]
0    742
5    511
1    359
dtype: int64

*** Feature weigths ***
 {'body_bow': 0.2, 'liwc_features': 0.0, 'lexicon_features': 0.0, 'surface_features': 0.0, 'pos_features': 0.0, 'readability_features': 0.0}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'test_score': array([ 0.94602699,  0.94898725,  0.94111028]), 'score_time': array([ 244.07621408,  235.800946  ,  245.89047503]), 'fit_time': array([ 494.43403196,  481.66572213,  487.61965919]), 'train_score': array([ 0.97505626,  0.97375328,  0.97619048])}
 Mean train score:  0.975000007032
 Mean test score:  0.945374836963
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.98      0.97      0.98      4000
           1       0.97      0.98      0.98      4000

   micro avg       0.98      0.98      0.98      8000
   macro avg       0.98      0.98      0.98      8000
weighted avg       0.98      0.98      0.98      8000

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.67      0.25      0.36        48
           1       0.51      0.86      0.64        44

   micro avg       0.54      0.54      0.54        92
   macro avg       0.59      0.56      0.50        92
weighted avg       0.59      0.54      0.50        92

confusion matrix:
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.94      0.97        33

   micro avg       0.94      0.94      0.94        33
   macro avg       0.50      0.47      0.48        33
weighted avg       1.00      0.94      0.97        33

confusion matrix:
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.72      0.20      0.31       250
           1       0.54      0.92      0.68       250

   micro avg       0.56      0.56      0.56       500
   macro avg       0.63      0.56      0.50       500
weighted avg       0.63      0.56      0.50       500

confusion matrix:
Test results on data sampled from emergent dataset (a broad distribution acc. to topic modeling -- possibly some overlapping claims):
[[  0 742]
 [  1 359]
 [  5 511]]
Discarding items for label 5
Final size of dataset:
[[  0 742]
 [  1 359]]
Final size of remaining dataset:
[[  5 511]]
              precision    recall  f1-score   support

           0       0.78      0.37      0.50       742
           1       0.38      0.79      0.51       359

   micro avg       0.50      0.50      0.50      1101
   macro avg       0.58      0.58      0.50      1101
weighted avg       0.65      0.50      0.50      1101


*** Feature weigths ***
 {'body_bow': 0.2, 'liwc_features': 0.2, 'lexicon_features': 0.2, 'surface_features': 0.0, 'pos_features': 0.0, 'readability_features': 0.0}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'test_score': array([ 0.93065967,  0.9336084 ,  0.93623406]), 'score_time': array([ 231.06094217,  231.23755383,  234.66984701]), 'fit_time': array([ 474.40636182,  476.43637204,  454.26216006]), 'train_score': array([ 0.96286572,  0.96512936,  0.96587927])}
 Mean train score:  0.964624780117
 Mean test score:  0.93350071026
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.97      0.96      0.97      4000
           1       0.96      0.97      0.97      4000

   micro avg       0.97      0.97      0.97      8000
   macro avg       0.97      0.97      0.97      8000
weighted avg       0.97      0.97      0.97      8000

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.61      0.23      0.33        48
           1       0.50      0.84      0.63        44

   micro avg       0.52      0.52      0.52        92
   macro avg       0.56      0.54      0.48        92
weighted avg       0.56      0.52      0.47        92

confusion matrix:
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.94      0.97        33

   micro avg       0.94      0.94      0.94        33
   macro avg       0.50      0.47      0.48        33
weighted avg       1.00      0.94      0.97        33

confusion matrix:
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
              precision    recall  f1-score   support

           0       0.71      0.22      0.34       250
           1       0.54      0.91      0.68       250

   micro avg       0.56      0.56      0.56       500
   macro avg       0.62      0.56      0.51       500
weighted avg       0.62      0.56      0.51       500

confusion matrix:
Test results on data sampled from emergent dataset (a broad distribution acc. to topic modeling -- possibly some overlapping claims):
[[  0 742]
 [  1 359]
 [  5 511]]
Discarding items for label 5
Final size of dataset:
[[  0 742]
 [  1 359]]
Final size of remaining dataset:
[[  5 511]]
              precision    recall  f1-score   support

           0       0.79      0.43      0.56       742
           1       0.39      0.77      0.52       359

   micro avg       0.54      0.54      0.54      1101
   macro avg       0.59      0.60      0.54      1101
weighted avg       0.66      0.54      0.54      1101


*** Feature weigths ***
 {'body_bow': 0.2, 'liwc_features': 0.2, 'lexicon_features': 0.2, 'surface_features': 0.2, 'pos_features': 0.2, 'readability_features': 0.2}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'test_score': array([ 0.8808096 ,  0.90810203,  0.88109527]), 'score_time': array([ 219.05679893,  213.97985291,  227.51178789]), 'fit_time': array([ 447.92955494,  453.295192  ,  438.92490411]), 'train_score': array([ 0.89816204,  0.93269591,  0.90682415])}
 Mean train score:  0.912560700168
 Mean test score:  0.890002298176
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support

           0       0.94      0.93      0.93      4000
           1       0.93      0.94      0.94      4000

   micro avg       0.94      0.94      0.94      8000
   macro avg       0.94      0.94      0.93      8000
weighted avg       0.94      0.94      0.93      8000

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.56      0.69      0.62        48
           1       0.55      0.41      0.47        44

   micro avg       0.55      0.55      0.55        92
   macro avg       0.55      0.55      0.54        92
weighted avg       0.55      0.55      0.55        92

confusion matrix:
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.88      0.94        33

   micro avg       0.88      0.88      0.88        33
   macro avg       0.50      0.44      0.47        33
weighted avg       1.00      0.88      0.94        33

confusion matrix:
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
              precision    recall  f1-score   support

           0       0.55      0.65      0.60       250
           1       0.57      0.48      0.52       250

   micro avg       0.56      0.56      0.56       500
   macro avg       0.56      0.56      0.56       500
weighted avg       0.56      0.56      0.56       500

confusion matrix:
Test results on data sampled from emergent dataset (a broad distribution acc. to topic modeling -- possibly some overlapping claims):
[[  0 742]
 [  1 359]
 [  5 511]]
Discarding items for label 5
Final size of dataset:
[[  0 742]
 [  1 359]]
Final size of remaining dataset:
[[  5 511]]
              precision    recall  f1-score   support

           0       0.72      0.81      0.76       742
           1       0.47      0.36      0.41       359

   micro avg       0.66      0.66      0.66      1101
   macro avg       0.60      0.58      0.58      1101
weighted avg       0.64      0.66      0.65      1101


*** Feature weigths ***
 {'body_bow': 0.2, 'liwc_features': 0.2, 'lexicon_features': 0.2, 'surface_features': 0.2, 'pos_features': 0.0, 'readability_features': 0.0}
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Fitting the model...
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
********************


{'test_score': array([ 0.87481259,  0.8735934 ,  0.92985746]), 'score_time': array([ 219.72436905,  213.95818901,  227.91107798]), 'fit_time': array([ 446.38129497,  452.42315197,  437.77403808]), 'train_score': array([ 0.8944111 ,  0.88713911,  0.95294338])}
 Mean train score:  0.911497864155
 Mean test score:  0.892754485473
********************


Inside the init function of PosTagFeatures()
Inside the init function of SurfaceFeatures()
Inside the init function of LexiconFeatures()
Inside the init function of LiwcFeatures()
Inside the init function of readabilityFeatures()
Results on training data:
              precision    recall  f1-score   support
/Users/fa/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

           0       0.87      0.86      0.86      4000
           1       0.86      0.87      0.86      4000

   micro avg       0.86      0.86      0.86      8000
   macro avg       0.86      0.86      0.86      8000
weighted avg       0.86      0.86      0.86      8000

Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):
[[ 0 48]
 [ 1 44]
 [ 5 26]]
Discarding items for label 5
Final size of dataset:
[[ 0 48]
 [ 1 44]]
Final size of remaining dataset:
[[ 5 26]]
              precision    recall  f1-score   support

           0       0.78      0.38      0.51        48
           1       0.57      0.89      0.69        44

   micro avg       0.62      0.62      0.62        92
   macro avg       0.67      0.63      0.60        92
weighted avg       0.68      0.62      0.59        92

confusion matrix:
Test results on data sampled only from buzzfeedTop (mixed claims):
[[ 1 33]]
Final size of dataset:
[[ 1 33]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.82      0.90        33

   micro avg       0.82      0.82      0.82        33
   macro avg       0.50      0.41      0.45        33
weighted avg       1.00      0.82      0.90        33

confusion matrix:
Test results on data sampled only from perez (celebrity stories):
[[  0 250]
 [  1 250]]
Final size of dataset:
[[  0 250]
 [  1 250]]
Final size of remaining dataset:
[]
              precision    recall  f1-score   support

           0       0.74      0.17      0.28       250
           1       0.53      0.94      0.68       250

   micro avg       0.56      0.56      0.56       500
   macro avg       0.64      0.56      0.48       500
weighted avg       0.64      0.56      0.48       500

confusion matrix:
Test results on data sampled from emergent dataset (a broad distribution acc. to topic modeling -- possibly some overlapping claims):
[[  0 742]
 [  1 359]
 [  5 511]]
Discarding items for label 5
Final size of dataset:
[[  0 742]
 [  1 359]]
Final size of remaining dataset:
[[  5 511]]
              precision    recall  f1-score   support

           0       0.75      0.40      0.52       742
           1       0.37      0.73      0.49       359

   micro avg       0.51      0.51      0.51      1101
   macro avg       0.56      0.56      0.51      1101
weighted avg       0.63      0.51      0.51      1101


Process finished with exit code 0












## Used ideas from https://www.kaggle.com/metadist/work-like-a-pro-with-pipelines-and-feature-unions
## Used ideas from https://www.kaggle.com/edolatabadi/feature-union-with-grid-search
## Used ideas from https://github.com/scikit-learn/scikit-learn/issues/6122 feature selection output



from __future__ import print_function

import numpy as np

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.datasets import fetch_20newsgroups
from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer
from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from collections import Counter
from sklearn.model_selection import cross_validate


from textutils import DataLoading
import os
import numpy as np
from nltk.corpus import stopwords
import nltk
import pandas as pd
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import GridSearchCV
from sklearn.externals import joblib
import textstat
import string

#Reproducibility
import random
np.random.seed(0)
random.seed(0)




#################

print("Preparing lexicons & lwicDic")
lexicon_directory = "../data/bias_related_lexicons"
lexicons = []
lexiconNames = []
# print("LexiconFeatures() init: loading lexicons")
for filename in os.listdir(lexicon_directory):
    if filename.endswith(".txt"):
        file = os.path.join(lexicon_directory, filename)
        words = open(file, encoding = "ISO-8859-1").read()
        lexicon = {k: 0 for k in nltk.word_tokenize(words)}
        print("Number of terms in the lexicon " + filename + " : " + str(len(lexicon)))
        lexicons.append(lexicon)
        lexiconNames.append(filename)
        continue
    else:
        continue

liwcFile = "../data/lwic/vocabliwc_cats.csv"
cols = list(pd.read_csv(liwcFile, nrows=1))
df = pd.read_csv(liwcFile, index_col="Source (A)",
                 usecols=[i for i in cols if i not in ['WC', 'Analytic', 'Clout', 'Authentic', 'Tone', 'WPS']])
df = df.T
keys = df.index
df = df.reset_index().drop('index', axis='columns')
cols = df.columns
values = df.apply(lambda x: x > 0).apply(lambda x: list(cols[x.values]), axis=1)
liwcDic = dict(zip(keys, values))

#####################
class PosTagFeatures(BaseEstimator, TransformerMixin):
    pos_family = {}

    def __init__(self):
        print("Inside the init function of PosTagFeatures()")

    # fit() doesn't do anything, this is a transformer class
    def fit(self, texts, y=None):
        return self

    # all the work is done here
    def transform(self, texts):
        allTags = []
        tokenizer = lambda x: x.split()
        features = [dict(Counter(allTags + [tag for word, tag in nltk.pos_tag(tokenizer(text), tagset='universal')]))
                    for text in texts]
        # normalize by the number of all tags (words in the text + 12 smoothing factor)
        features = [{key: val / (len(text.split()) + 12) for key, val in d.items()} for text, d in zip(texts, features)]
        features = np.array(features)
        return features


class SurfaceFeatures(BaseEstimator, TransformerMixin):
    """Extract features from each document for DictVectorizer"""
    XXX = None

    def __init__(self):
        self.XXX = "Inside the init function of SurfaceFeatures()"
        print(self.XXX)

    def fit(self, x, y=None):
        return self

    def transform(self, posts):
        # posts[['feature1','feature2',...,'feature100']].to_dict('records')[0].to_dict('records')
        features = [{'num_char': len(text),
                     'num_sentence': text.count('.'),
                     'num_punc/num_char': len("".join(_ for _ in text if _ in string.punctuation)) / (len(text) + 1),
                     'num_upper/num_char': len([wrd for wrd in text.split() if wrd.isupper()]) / (len(text) + 1),
                     'num_word/num_sentence': len(text.split()) / (text.count('.') + 1)
                     }
                    for text in posts]
        return features


class LiwcFeatures(BaseEstimator, TransformerMixin):
    liwcDic = {}  # = map of text to dataframe row (this dataframe should be read from the file including liwc features)

    def __init__(self):
        print("Inside the init function of LiwcFeatures()")
        self.liwcDic = liwcDic

    def fit(self, x, y=None):
        return self

    def transform(self, texts):
        textvecs = []
        for text in texts:
            # print "*** Current text:\n" + text  + "\n***"
            tokens = nltk.word_tokenize(text)
            textvec = {}
            for category in self.liwcDic.keys():
                lexicon_words = [i for i in tokens if i in self.liwcDic[category]]
                count = len(lexicon_words)
                count = count * 1.0 / len(tokens)  # Continous treatment
                # count = 1 if (count > 0) else 0     #Binary treatment
                textvec[category] = count
            textvecs.append(textvec)
        textvecs = np.array(textvecs)
        return textvecs


class LexiconFeatures(BaseEstimator, TransformerMixin):
    lexicons = None
    lexiconNames = None

    def __init__(self):
        print("Inside the init function of LexiconFeatures()")
        self.lexicons = lexicons
        self.lexiconNames = lexiconNames

    def fit(self, x, y=None):
        return self

    def transform(self, texts):
        stoplist = stopwords.words('english')
        textvecs = []
        for text in texts:
            tokens = nltk.word_tokenize(text)
            textvec = {}
            for lexicon, lexiconName in zip(self.lexicons, self.lexiconNames):
                lexicon_words = [i for i in tokens if i in lexicon]
                count = len(lexicon_words)
                count = count * 1.0 / len(tokens)  # Continous treatment
                # count = 1 if (count > 0) else 0     #Binary treatment
                textvec[lexiconName] = count
            textvecs.append(textvec)
        textvecs = np.array(textvecs)
        return textvecs


class ReadabilityFeatures(BaseEstimator, TransformerMixin):
    def __init__(self):
        print("Inside the init function of readabilityFeatures()")

    def fit(self, x, y=None):
        return self

    def transform(self, posts):
        # posts[['feature1','feature2',...,'feature100']].to_dict('records')[0].to_dict('records')
        features = [{'flesch_reading_ease': textstat.flesch_reading_ease(text),
                     'smog_index': textstat.smog_index(text),
                     'flesch_kincaid_grade': textstat.flesch_kincaid_grade(text),
                     'coleman_liau_index': textstat.coleman_liau_index(text),
                     'automated_readability_index': textstat.automated_readability_index(text),
                     'dale_chall_readability_score': textstat.dale_chall_readability_score(text),
                     'linsear_write_formula': textstat.linsear_write_formula(text),
                     'gunning_fog': textstat.gunning_fog(text)
                     # 'text_standard': textstat.text_standard(text)
                     }
                    for text in posts]
        return features


class SubjectBodyExtractor(BaseEstimator, TransformerMixin):
    def fit(self, x, y=None):
        return self

    def transform(self, posts):
        # construct object dtype array with two columns
        # first column = 'subject' and second column = 'body'
        features = np.empty(shape=(len(posts), 2), dtype=object)
        for i, text in enumerate(posts):
            features[i, 0] = text[0:100]
            features[i, 1] = text
        return features




LOAD_DATA_FROM_DISK = True
CLASSES = 2


if LOAD_DATA_FROM_DISK:
	texts_train = np.load("../dump/trainRaw_rashkin")
	texts_test = np.load("../dump/testRaw")
	labels_train = np.load("../dump/trainlRaw_rashkin")
	labels_test = np.load("../dump/testlRaw")
	print("Data loaded from disk!")

else:
	# Data sources used for training:
	texts_train_rashkin, labels_train_rashkin = DataLoading.load_data_rashkin("../data/rashkin/xtrain.txt", CLASSES )#load_data_liar("../data/liar_dataset/train.tsv")#load_data_rashkin("../data/r$
	print(len(texts_train_rashkin))
	texts_train, labels_train, texts, labels = DataLoading.balance_data(texts_train_rashkin, labels_train_rashkin, 4000, [4])
	texts_train.dump("../dump/trainRaw_rashkin")
	labels_train.dump("../dump/trainlRaw_rashkin")
	print("Data dumped to disk!")

print("Size of train and test sets: " + str(len(labels_train)) + " , "  +  str(len(labels_test)))
texts_train_valid = texts_train
labels_train_valid = labels_train
print(texts_train_valid[0:3][0:10])
print(labels_train_valid[0:3])




# Train & Test Loop

tws = [

    {'body_bow': 0.2,
            'surface_features': 0.0,
            'lexicon_features': 0.0,
            'liwc_features' :0.0,
            'pos_features':0.0,
            'readability_features': 0.0},
    {'body_bow': 0.2,
            'surface_features': 0.0,
            'lexicon_features': 0.2,
            'liwc_features' :0.2,
            'pos_features':0.0,
            'readability_features': 0.0},
    {'body_bow': 0.2,
            'surface_features': 0.2,
            'lexicon_features': 0.2,
            'liwc_features' :0.2,
            'pos_features':0.2,
            'readability_features': 0.2},
    {'body_bow': 0.2,
         'surface_features': 0.2,
         'lexicon_features': 0.2,
         'liwc_features': 0.2,
         'pos_features': 0.0,
         'readability_features': 0.0}
]

'''
    {'body_bow': 0.2,
            'surface_features': 0.0,
            'lexicon_features': 0.2,
            'liwc_features' :0.2,
            'pos_features':0.2,
            'readability_features': 0.0},
    {'body_bow': 0.2,
            'surface_features': 0.2,
            'lexicon_features': 0.2,
            'liwc_features' :0.0,
            'pos_features':0.2,
            'readability_features': 0.0},
    {'body_bow': 0.2,
     'surface_features': 0.2,
     'lexicon_features': 0.2,
     'liwc_features': 0.0,
     'pos_features': 0.0,
     'readability_features': 0.0},
    {'body_bow': 0.2,
     'surface_features': 0.2,
     'lexicon_features': 0.0,
     'liwc_features': 0.0,
     'pos_features': 0.2,
     'readability_features': 0.0},
    {'body_bow': 0.2,
     'surface_features': 0.0,
     'lexicon_features': 0.2,
     'liwc_features': 0.0,
     'pos_features': 0.2,
     'readability_features': 0.0},
    {'body_bow': 0.2,
     'surface_features': 0.2,
     'lexicon_features': 0.2,
     'liwc_features': 0.2,
     'pos_features': 0.2,
     'readability_features': 0.0},
    {'body_bow': 0.2,
     'surface_features': 0.2,
     'lexicon_features': 0.2,
     'liwc_features': 0.2,
     'pos_features': 0.0,
     'readability_features': 0.0},
    {'body_bow': 0.2,
            'surface_features': 0.0,
            'lexicon_features': 0.2,
            'liwc_features' :0.2,
            'pos_features':0.0,
            'readability_features': 0.0},
    {'body_bow': 0.2,
        'surface_features': 0.2,
        'lexicon_features': 0.2,
        'liwc_features' :0.2,
        'pos_features':0.2,
        'readability_features': 0.2},
    {'body_bow': 0.2,
     'surface_features': 0.0,
     'lexicon_features': 0.0,
     'liwc_features': 0.0,
     'pos_features': 0.0,
     'readability_features': 0.0},
    {'body_bow': 0.0,
        'surface_features': 0.2,
        'lexicon_features': 0.0,
        'liwc_features' :0.0,
        'pos_features':0.0,
        'readability_features': 0.0},
    {'body_bow': 0.0,
        'surface_features': 0.0,
        'lexicon_features': 0.2,
        'liwc_features' :0.0,
        'pos_features':0.0,
        'readability_features': 0.0},
    {'body_bow': 0.0,
        'surface_features': 0.0,
        'lexicon_features': 0.0,
        'liwc_features' :0.2,
        'pos_features':0.0,
        'readability_features': 0.0},
    {'body_bow': 0.0,
        'surface_features': 0.0,
        'lexicon_features': 0.0,
        'liwc_features' :0.0,
        'pos_features':0.2,
        'readability_features': 0.0},
    {'body_bow': 0.0,
        'surface_features': 0.0,
        'lexicon_features': 0.0,
        'liwc_features' :0.0,
        'pos_features':0.0,
        'readability_features': 0.2}
'''




texts_snopesChecked, labels_snopesChecked = DataLoading.load_data_snopes("../data/snopes/snopes_checked_v02_right_forclassificationtest.csv",CLASSES)
texts_buzzfeedTop, labels_buzzfeedTop = DataLoading.load_data_buzzfeedtop()
texts_perez, labels_perez = DataLoading.load_data_perez("../data/perez/celeb.csv")
texts_emergent, labels_emergent = DataLoading.load_data_emergent()

for tw in tws:
    print("\n*** Feature weigths *** \n", tw)
    pipeline = Pipeline([
        # Extract the subject & body
        ('subjectbody', SubjectBodyExtractor()),

        # Use ColumnTransformer to combine the features from subject and body
        ('union', ColumnTransformer(
            [
                ('body_bow', TfidfVectorizer(sublinear_tf=True, max_df=0.5, min_df=5, ngram_range=(1 ,2),
                                             stop_words='english'), 1),
                ('pos_features', Pipeline([
                    ('stats', PosTagFeatures()),  # returns a list of dicts
                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix
                ]), 1),
                ('surface_features', Pipeline([
                    ('stats', SurfaceFeatures()),  # returns a list of dicts
                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix
                ]), 1),
                ('lexicon_features', Pipeline([
                    ('stats', LexiconFeatures()),  # returns a list of dicts
                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix
                ]), 1),
                ('liwc_features', Pipeline([
                    ('stats', LiwcFeatures()),  # returns a list of dicts
                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix
                    ]), 1),
                ('readability_features', Pipeline([
                    ('stats', ReadabilityFeatures()),  # returns a list of dicts
                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix
                    ]), 1)
            ],
            transformer_weights=tw
        )),
        ('svc', LinearSVC(penalty= "l2", dual=False, tol=1e-3)),
    ])


    print("Fitting the model...")

    cross_val = cross_validate(pipeline, texts_train_valid, labels_train_valid, return_train_score=True, cv=3, scoring = 'f1_micro')
    print("********************\n\n")
    print(cross_val, "\n Mean train score: ", np.mean(cross_val['train_score']), "\n Mean test score: ", np.mean(cross_val['test_score']))
    print("********************\n\n")
    grid_search = pipeline.fit(texts_train_valid, labels_train_valid)
    print("Results on training data:")
    y = grid_search.predict(texts_train_valid)
    print(classification_report(labels_train_valid, y))

    print("Test results on data sampled only from snopes (snopes312 dataset manually checked right items -- unseen claims):")
    texts_test, labels_test, texts, labels = DataLoading.balance_data(texts_snopesChecked, labels_snopesChecked ,sample_size=None , discard_labels=[2,5])
    y = grid_search.predict(texts_test)
    print(classification_report(labels_test, y))
    print("confusion matrix:")


    print("Test results on data sampled only from buzzfeedTop (mixed claims):")
    texts_test, labels_test, texts, labels = DataLoading.balance_data(texts_buzzfeedTop, labels_buzzfeedTop ,sample_size=None , discard_labels=[])
    y = grid_search.predict(texts_test)
    print(classification_report(labels_test, y))
    print("confusion matrix:")


    print("Test results on data sampled only from perez (celebrity stories):")
    texts_test, labels_test, texts, labels = DataLoading.balance_data(texts_perez, labels_perez ,sample_size=None , discard_labels=[])
    y = grid_search.predict(texts_test)
    print(classification_report(labels_test, y))
    print("confusion matrix:")

    print("Test results on data sampled from emergent dataset (a broad distribution acc. to topic modeling -- possibly some overlapping claims):")
    texts_test, labels_test, texts, labels = DataLoading.balance_data(texts_emergent, labels_emergent , None, [2,5])
    y = grid_search.predict(texts_test)
    print(classification_report(labels_test,y))



